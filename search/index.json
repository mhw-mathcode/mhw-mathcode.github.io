[{"content":"","date":"2025-06-22T11:50:20+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"学习笔记 数据结构"},{"content":"开个坑，主要是记录后续复习操作系统的一些笔记与完成 6.S081 课程的 lab 。\n","date":"2025-06-17T18:27:49+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"学习笔记-操作系统"},{"content":"0 算法速览 监督学习算法：\n线性回归（Linear Regression）：用于回归任务，预测连续的数值。 逻辑回归（Logistic Regression）：用于二分类任务，预测类别。 决策树（Decision Tree）：基于树状结构进行决策的分类或回归方法。 支持向量机（SVM）：用于分类任务，构建超平面进行分类。 K近邻算法 集成学习 无监督学习算法：\nK-means 聚类：通过聚类中心将数据分组。 主成分分析（PCA）：用于降维，提取数据的主成分。 0.1 线性回归 线性回归 (Linear Regression) 是一种用于预测连续值的最基本的机器学习算法，它假设目标变量 y 和特征变量 x 之间存在线性关系，并试图找到一条最佳拟合直线来描述这种关系。\n常用的误差函数是均方误差 (MSE) : $MSE = 1/n * Σ(y_i - y_{pred_i})^2$\n求解方法 1. 最小二乘法 最小二乘法的目标是最小化残差平方和（RSS），其公式为：$RSS = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$\n得到最佳的 $w, b$\n$$ \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i^2 \u0026 \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i \u0026 n \\end{bmatrix}^{-1} \\begin{bmatrix} \\sum_{i=1}^n x_i y_i \\\\ \\sum_{i=1}^n y_i \\end{bmatrix} $$2. 梯度下降法 梯度下降法的目标是最小化损失函数 $J(w,b)$ 。对于线性回归问题，通常使用均方误差（MSE）作为损失函数：\n$$ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$参数更新：\n$$ w := w - \\alpha \\frac{\\partial J}{\\partial w} \\quad b := b - \\alpha \\frac{\\partial J}{\\partial b} $$梯度下降法的步骤\n初始化参数：初始化 w 和 b 的值（通常设为 0 或随机值）。 计算损失函数：计算当前参数下的损失函数值 $J(w,b)$ 。 计算梯度：计算损失函数对 w 和 b 的偏导数。 更新参数：根据梯度更新 w 和 b。 重复迭代：重复步骤 2 到 4，直到损失函数收敛或达到最大迭代次数。 0.2 逻辑回归 逻辑回归（Logistic Regression）是一种广泛应用于分类问题的统计学习方法，尽管名字中带有\u0026quot;回归\u0026quot;，但它实际上是一种用于二分类或多分类问题的算法。\n逻辑回归通过使用逻辑函数（也称为 Sigmoid 函数）将线性回归的输出映射到 0 和 1 之间，从而预测某个事件发生的概率。建立模型：\n\\[ p(y = 1|X) = \\sigma(w^T X + b) \\]其中：\n\\( X \\) 是输入特征（可以是多个特征组成的向量）。 \\( w \\) 是权重向量。 \\( b \\) 是偏置项。 \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) 是Sigmoid函数。Sigmoid函数将模型的输出值 \\((w^T X + b)\\) 映射到0到1之间，因此它可以看作是属于类别1的概率。注意 $\\sigma\u0026rsquo;(z) = \\sigma(z)(1 - \\sigma(z))$ 。 使用对数损失函数\n\\[ L(\\theta) = \\begin{cases} -\\log(p), \u0026 \\text{if } y = 1 \\\\ -\\log(1-p), \u0026 \\text{if } y = 0 \\end{cases} \\]合并得到单个样品的损失函数\n\\[ L(\\theta) = -ylog(p)-(1-y)log(1-p) \\]因此总体损失函数（也就是交叉熵损失函数）\n\\[ L(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(p^{(i)}) + (1 - y^{(i)}) \\log(1 - p^{(i)}) \\right] \\]求解方法 梯度下降法 对 \\(w\\) 的梯度： \\[ \\frac{\\partial J(w, b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\]对 \\(b\\) 的梯度： \\[ \\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\]\n0.3 决策树 决策树（Decision Tree），它是一种以树形数据结构来展示决策规则和分类结果的模型，作为一种归纳学习算法，其重点是将看似无序、杂乱的已知数据，通过某种技术手段将它们转化成可以预测未知数据的树状模型，每一条从根结点（对最终分类结果贡献最大的属性）到叶子结点（最终分类结果）的路径都代表一条决策的规则。\n在这个过程中，寻找最优划分属性是决策树过程中的重点，那么应该如何求解呢？\n求解方法 1. 信息增益 首先引入信息熵的概念\n信息熵：描述随机变量的不确定性（也就是混乱程度）。 假设某随机变量的概率分布为：$P(X=x_i) = p_i, \\quad i = 1, 2, \\ldots, n$ ，则它的信息熵计算公式为：$H(X) = - \\sum_{i=1}^{n} p_i \\log p_i$\n在决策树中，信息熵\n\\[ H(D) = - \\sum_{k=1}^{K} \\frac{|D_k|}{|D|} \\log \\frac{|D_k|}{|D|} \\] \\( D \\)：整个数据集 \\( D_k \\)：第 \\( k \\) 个类的样本子集 \\( \\frac{|D_k|}{|D|} \\)：第 \\( k \\) 类的概率 条件熵\n\\[ H(D|A) = \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} H(D_i) \\]其中 \\( H(D_i) \\) 是每个子集 \\( D_i \\) 的信息熵\n\\[ H(D_i) = - \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|} \\log \\frac{|D_{ik}|}{|D_i|} \\] \\( A \\)：某个属性 \\( D_i \\)：属性 \\( A \\) 的第 \\( i \\) 个取值所对应的数据子集 \\( D_{ik} \\)：在第 \\( i \\) 个子集中属于第 \\( k \\) 类的样本数 因此有\n\\[ H(D|A) = - \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|} \\log \\frac{|D_{ik}|}{|D_i|} \\]特征 \\( A \\) 对训练数据集 \\( D \\) 的信息增益 \\( gain(D, A) \\) 定义为集合 \\( D \\) 的信息熵 \\( H(D) \\) 与特征 \\( A \\) 给定条件下 \\( D \\) 的信息条件熵 \\( H(D|A) \\) 之差，即公式为：\n\\[ gain(D, A) = H(D) - H(D|A) \\]信息增益表示得知特征 \\( X \\) 的信息而使得类 \\( Y \\) 的信息的不确定性减少的程度，因此信息增益最大的属性就是最优划分属性，标志性算法 $ID3$ 。\n2. 增益比 信息增益虽然在理论上可以找到最优的划分属性，但在某些情况下会存在问题。信息增益比较偏好可取值较多的属性。因此为了矫正信息增益偏好的问题，使算法不偏向可取值较多的属性，引申出了增益比的思想。\n\\[ Gain\\_ratio(D, A) = \\frac{Gain(D, A)}{H(A)} \\]可以看出，增益比就是信息增益除以属性 $A$ 的信息熵，当属性 $A$ 可取值增多的时候，$H(A)$ 一般也增大，因此在一定程度上能抑制信息增益偏好取值多的属性的特点，但是增益比偏好取值较少的属性。\n算法 $C4.5$ 是算法 $ID3$ 的改进版，它使用了信息增益和增益比两种选择算法，先选出信息增益高于平均水平的属性，然后再在这些属性中选择增益比最高的，作为最优划分属性。这样综合了信息增益和增益比的优点，可以取得较好的效果。\n3. 基尼指数 基尼指数是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。这个规律与信息熵的相同。\n\\[ Gini(D) = \\sum_{k=1}^{n} \\sum_{k' \\neq k} p_k p_{k'} = 1 - \\sum_{k=1}^{n} p_k^2 \\]使用基尼指数来选择最优划分属性也是对比不同属性划分后基尼指数的差值，选择使样本集基尼指数减小最多的属性。\n\\[ Gain(D, a) = Gini(D) - \\sum_{i=1}^{n} \\frac{|D^i|}{|D|} Gini(D^i) \\]著名的 $CART$ 决策树就是使用基尼指数来作为划分准则， $CART$ 决策树与 $ID3$ 和 $C4.5$ 的区别：\n划分准则不同，CART决策树使用基尼指数， $ID3$ 和 $C4.5$ 使用信息熵。\n$ID3$ 和 $C4.5$ 划分时，一个节点可以划分为多个子结点，子结点数量根据属性可取值的数量决定。而 $CART$ 决策树是严格的二叉树结构，就是说 $1$ 个节点最多划分为 $2$ 子结点。\n0.4 支持向量机 支持向量机（Support Vector Machine，简称 SVM）是一种监督学习算法，主要用于分类和回归问题。\nSVM 的核心思想是找到一个最优的超平面，将不同类别的数据分开。这个超平面不仅要能够正确分类数据，还要使得两个类别之间的间隔（margin）最大化。\n支持向量：支持向量是离超平面最近的样本点。这些支持向量对于定义超平面至关重要。支持向量机通过最大化支持向量到超平面的距离（即最大化间隔）来选择最佳的超平面。\n当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机；\n当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机；\n当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机。\n求解方法 间隔最大化和支持向量 好难……\n0.5 K近邻算法 K 近邻算法（K-Nearest Neighbors，简称 KNN）是一种简单且常用的分类和回归算法。\nK 近邻算法属于监督学习的一种，核心思想是通过计算待分类样本与训练集中各个样本的距离，找到距离最近的 K 个样本，然后根据这 K 个样本的类别或值来预测待分类样本的类别或值。\n求解方法 基本步骤：\n计算距离：计算待分类样本与训练集中每个样本的距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。\n选择 K 个最近邻：根据计算出的距离，选择距离最近的 K 个样本。\n投票或平均：对于分类问题，K 个最近邻中出现次数最多的类别即为待分类样本的类别；对于回归问题，K 个最近邻的值的平均值即为待分类样本的值。\n0.6 集成学习 0.7 K-means 聚类 K-means 聚类是一种常用的基于距离的聚类算法，旨在将数据集划分为 K 个簇。算法的目标是最小化簇内的点到簇中心的距离总和。\n求解方法 基本步骤：\n选择 $K$ 值：设定簇的数量 。\n初始化簇中心：随机选择 $K$ 个数据点作为初始簇中心（centroids）。\n分配步骤（Assignment Step）：对于数据集中的每个点，将它分配到最近的簇中心对应的簇。这里的“距离”通常使用欧氏距离（Euclidean distance）。\n更新步骤（Update Step）：根据当前的簇分配，重新计算每个簇的中心，即计算簇内所有点的均值作为新的簇中心。\n重复 3 和 4 步：不断重复分配和更新步骤，直到簇中心不再发生变化（收敛）或达到指定的最大迭代次数。\n确定最佳的簇数 $K$ 是 $K-means$ 聚类中的一个难点。聚类的目标是使得每个样本点到距离其最近的聚类中心的总误差平方和（也即聚类的代价函数，记作 $SSE$ ）尽可能小。\n空间中数据对象与聚类中心间的欧式距离计算公式为：\n\\[ d(x, C_i) = \\sqrt{\\sum_{j=1}^{m} (x_j - C_{ij})^2} \\]其中：\n\\( x \\) 为数据对象， \\( C_i \\) 为第 \\( i \\) 个聚类中心， \\( m \\) 为数据对象的维度， \\( x_j \\)，\\( C_{ij} \\) 为 \\( x \\) 和 \\( C_i \\) 的第 \\( j \\) 个属性值。 整个数据集的误差平方和 SSE 计算公式为：\n\\[ SSE = \\sum_{i=1}^{k} \\sum_{x \\in C_i} |d(x, C_i)|^2 \\]其中：\nSSE 的大小表示聚类结果的好坏， \\( k \\) 为簇的个数。 理论上随着 $K$ 的增加， $SSE$ 会单调递减，当 $K$ 超过某一个数后，每个类簇的聚合程度不再获得显著提升，此时我们就可以认为已找到最佳 $K$ 的取值（肘部法）。\nK-means++ K-means++ 是一种改进的初始化方法，可以帮助选择更合理的初始中心，优先选择“距离最远”的点作为初始质心，减少陷入局部最优的风险。\n基本步骤：\n从数据集 $\\mathcal{X}$ 中随机（均匀分布）选取一个样本点作为第一个初始聚类中心;\n接着计算每个样本与当前已有聚类中心之间的最短距离，用 $D(x)$ 表示；然后计算每个样本点被选为下一个聚类中心的概率 $P(x) = \\frac{D(x)^2}{\\sum_{x \\in \\mathcal{X}} D(x)^2}$，最后选择最大概率值（或者概率分布）所对应的样本点作为下一个簇中心；\n重复步骤 2，直到选择 $K$ 个聚类中心\n优势：避免随机初始化，加快收敛速度，聚类结果更加稳定。\n0.8 主成分分析 主成分分析（PCA）是一种无监督学习方法，旨在通过线性变换将原始的高维数据映射到一个低维空间，同时尽可能保留数据的方差（即信息量）。简单来说，PCA 的目标是找到一组新的坐标轴（称为主成分），这些坐标轴能够捕捉数据中最大的变异性，并用更少的维度来近似表示原始数据。\n求解方法 数据中心化：首先将数据中心化，即让每个特征的均值变为 0。\n计算协方差矩阵： \\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\] 协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立。\n特征值分解：对协方差矩阵进行特征值分解，得到主成分的方向（特征向量）和重要性（特征值）。\n\\[ \\det(A - \\lambda I) = 0 \\] 其中：\n\\( A \\) 是协方差矩阵， \\( \\lambda \\) 是特征值， \\( I \\) 是单位矩阵。 得到特征值 \\( \\lambda \\) 以后代入 $(A - \\lambda I)v_1 = 0$ 解得特征向量 $v_1$ 。\n排序和选择主成分：将特征值从大到小排序，特征值最大的为第一个主成分，捕捉了数据中最大的变化，也就是数据分布中最显著的变化方向。第二个主成分与第一个主成分正交（相互垂直），且在正交约束下方差次大的方向。后续主成分：依此类推，每个主成分都与前面的主成分正交，并按特征值大小递减排列。\n投影数据：将中心化后的数据投影到第一个主成分上，得到降维后的结果。\n1 引言 1.1 什么是机器学习 一个好的学习问题定义如下：一个程序被认为能从经验 E 中学习，解决任务 T ，达到性能度量值 P ，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n目前存在几种不同类型的学习算法，其中主要的两种类型被我们称之为：监督学习和无监督学习。\n1.2 监督学习 监督学习：给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。\n回归 (regression) 问题：推测出这一系列连续值属性。\n分类 (classification) 问题：推测出离散的输出值。\n1.3 无监督学习 无监督学习：没有标签的数据，模型通过探索数据中的结构或模式来进行学习。\n聚类算法：将数据集划分成两个不同的簇。\n鸡尾酒算法：分离两种声音。（一个具体实例，仅仅只需要一行代码实现）\n1 [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x\u0026#39;); 2 单变量线性回归 (Linear Regression with One Variable) ","date":"2025-06-16T19:29:56+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","title":"学习笔记-机器学习"},{"content":"DAY1 因为队友有考试，所以我们集体在周六早晨赶的七点的飞机来昆明，当时在登机的时候遇到超级漂亮的天空，笑着和队友说这是大捷的前兆hh。\n然后来酒店办了入住直接冲向学校了，当时整个人都饿毁了，两荤三素库库炫，也没来得及拍照 。不过云大真的很漂亮，整个云南十二月还是绿叶环绕，本北方人表示哪见过这个。\n然后热身赛直接开打。A题a+b？B题队友猜一发结论直接过了。D题偷听斜对面什么父亲，然后随便一手模发现每个点权值减去父亲权值乘起来直接过了。C题队友一开始思路有点问题，后来lyr测点别的东西我俩直接玩手机了。结果快最后了zmd想到了正解，赛后一问黄队还真对了。\nDAY2 有点紧张！沈阳铜首确实给我们的压力太大了，赛前买了一大批物资，面包，士力架，红牛，赛前满足每位队员的一切合理与不合理的需求(bushi)\n开打！上来队友直接扔给我一道计算几何，说极角排序一下应该就完了。我觉得也是于是上机开始敲。小小调整一下交了结果wa了。此时队友上机写M构造，结果几种方式全都有问题。只能说逆风开局是我们队的常态，甚至比沈阳的台风开局还要好一点。然后zmd开始了无敌节奏，先想出了M超级正确且好写且可证明的思路，直接过了，然后上机又把J题秒了。期间lyr帮忙手推了J题小数据特例并且帮我找到了H题一个小讨论的错误（某人唯二贡献哈哈哈）。然后我上机改改也过了H。赛后想想这个H当时真的很唐，直接用atan2搞出角度其实就完了，根本用不到极角排序。\n然后有点卡题的，当时我说开出CGL其实是应该能稳银的（很棒的前瞻性），于是开始全力开这三题。lyr一直在想L，我看了C就觉得是根号分治，但是一直不知道分完怎么算。zmd张老板直接灵感一现，从结果往回推，轻松过样例！交上去却t了。然后下机想想就找到了问题，问我一个式子，（然后巴拉巴拉交流一堆），改一下直接过了！这个C开的很帅。然后我顺着zmd思路想G题，zmd写了一个记忆化搜索，re了，改了下wa了，好在很快找到反例发现比较难记忆，并且也改不对样例。这时候我就想是不是直接爆搜就完了，上机十分钟写完肯定过样例的，然后试了极限的俩数结果都跑的飞快？于是决定交一发结果直接ac了？？？我真的对这发没报希望啊？最后直接all in L题。zmd和lyr统一了思路，lyr写完交上去wa了，并且我造了一堆样例都没能出错。这时候张老板直接把lyr踹下机开始重构，微微调整过了第一个样例就觉得直接交，结果t了？当时三人盯着那几个while循环瞅，lyr建议关流在交一次（因为debug所以关了关流，交的时候忘开了），结果直接a了！当时张老板直接大喊：“过了！”我们直接下班观战！\n最后稳稳拿银~\n很不错的昆明，最后没有遗憾退役，赛后要了哥哥的签名和合照，美滋滋~\n晚上去一家特色菜馆，超级火爆，最后人均20？\n随便逛逛~\nDAY3 因为行李问题没机会去玩，然后偷了群友拍的鸽子的图片~\n晚上连夜飞回天津，晚上到了已经凌晨了，累鼠了。\nCCPC郑州站、ICPC沈阳站、ecfinal西安站就不转载了，一个是传奇赶路精疲力尽艰难守铜，一个是台风开局铜首结束道心破碎，一个是奖励名额纯纯旅游爽玩西安。\n","date":"2025-05-30T20:02:21+08:00","permalink":"https://mhw-mathcode.github.io/p/2024-icpc-%E6%98%86%E6%98%8E%E5%8C%BA%E5%9F%9F%E8%B5%9B-vlog/","title":"2024 ICPC 昆明区域赛 vlog"},{"content":"实验阶段暂不公开~\n","date":"2025-05-30T11:12:02+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E9%87%87%E6%A0%B7-%E6%83%85%E6%84%9F%E6%94%BB%E5%87%BB%E5%AE%9E%E9%AA%8C%E8%BF%9B%E5%BA%A6/","title":"大模型数据采样-情感攻击(实验进度)"},{"content":"前言（一些碎碎念） 高考结束以后什么也不懂的我报考了 cs 专业然后购入了一台集成显卡的电脑……大一的时候还好，只是知道带不动一些大型游戏，好在自己也并不打游戏。然后大二的时候 chatgpt 横空出世迅速占据了日常的生活，深度学习、神经网络直接闯入所有领域，在给各个领域注入新鲜血液与活力的同时，也让大家都多了一门必修课。越来越多的课程引入类似的知识与实验，当时也没有涉及很多大型的数据集与复杂的神经网络，勉强使用电脑的 cpu 运行应对实验。大三上知道了 google colab ，每天都有免费的 gpu 额度使用，甚至充钱可以用到 A100 （当然价格是相当高昂的）。大三下来到了学校的一个实验室做科研实习，LLM 相关的工作对于算力要求实在是很高，而且 colab 一些很不好的体验也难以满足需求，就找学长申请了一个实验室服务器的账号。在这个过程中也遇到了很多困难，也都找到了相应的答案，因此在这里记录一下。\n我使用的是 vscode + filezilla ，vscode 用于远程连接 + coding，filezilla 主要是传输数据。\n远程连接 ssh 首先学长会要你电脑主机的公钥，这篇文章 简洁而且有用，一般生成的公钥-私钥对会在这个路径下 C:/Users/用户名/.ssh 。\n然后使用 vscode 远程连接，需要用到这三个插件。\n然后如同 这篇文章 一样。\n但是我们操作以后可能会遇到一个问题：远程连接xxx失败！返回类似于以下形式的报错信息，也是卡了自己很长时间。\n1 2 3 Failed to parse remote port from server output Exec server for ssh-remote+xx.xx.xxx.xxx failed: Error Error opening exec server for ssh-remote+xx.xx.xx.xx: Error 然后 这篇文章 给出了一个完美的解决方案（膜拜）！总结原因就是 vscode 和 ubuntu 中 glibc 的版本不匹配，但是我们肯定是不好直接更改实验室服务器的系统的，所以对自己的 vscode 降级（降级到1.98及以下）就是最好的方案。\nFilezilla 传输数据 （其实可以直接拖拽完成，但是大规模数据还是用专业的软件比较好）\n然后我在连接之后又出现了一个问题，filezilla 死活连接不上，一直连接超时，然后自己也是摸索出了一个解决办法，因为看到很多博主在连接时会输入密钥的路径，但是自己却没有这个输入的选项，因此觉得应该是这个地方的问题。\n然后选择本地密钥所在的路径即可。\n连接成功以后直接把本地站点的文件拖进远程站点目标的文件夹下即可。\n配置环境 如果是实验室的服务器我觉得可能都已经安装好了Anaconda，并且已经创建并激活好了conda虚拟环境，如果是自己租赁的可能需要自己配置，参考 这篇文章 。\n然后我们直接初始化 python 环境即可。\n1 /opt/anaconda3/bin/conda init bash 重启终端后，会在用户名前出现 (base) 证明成功了，使用 conda info -e 查看 conda 环境。\n然后学长告诉我 /home/用户名 下的空间比较小，大的数据集放在 /data/用户名 。或许我们可以把所有东西（代码还有数据集）都放在 /data/用户名 下，这样会比较方便一点，具体参考 这篇文章 。\n紧接着又出现了一个问题。\nemmm，在研究尝试了一下午无果后，去问了一下学长，然后得到了答案：服务器没网。我应该早点去问学长的www，在学长的帮助下得到了解决方案：\n在服务器终端上运行\n1 export http_proxy=\u0026#34;http://$proxy_ip:$proxy_port\u0026#34; https_proxy=\u0026#34;http://$proxy_ip:$proxy_port\u0026#34; all_proxy=\u0026#34;socks5://$proxy_ip:$proxy_port\u0026#34; 其中 proxy_ip 和 proxy_port 分别代表代理服务器的 IP 地址和端口号（没错，代理服务器的 IP 和端口也是学长给的，我尝试了自己本地的代理没有成功）。\n然后我们直接创建并激活环境\n1 2 conda create -n mypytorch python=3.11 conda activate mypytorch 然后还需要安装最重要的 pytorch ，但是官网最新的版本好像不支持 conda 安装，于是我决定安装上一代版本。\nPyTorch previous-versions\n但是又又又又出现了新的问题：\n大概是因为 pytorch-cuda=12.4 因为版本比较新所以在当前配置的 Conda 通道中不可用，然后官方是提供了支持 CUDA 12.4 的安装命令：\n1 conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia 然后，终于！\n得到了 pytorch 版本号！\ntrick trick (1) 控制使用的显卡 id\n1 2 import os os.environ[\u0026#39;CUDA_VISIBLE_DEVICES\u0026#39;] = \u0026#39;1\u0026#39; trick (2) 使用 jupyter\n终端执行命令 conda install jupyter\nvscode 中安装插件 jupyter\ntrick (3) 在连接服务器的 vscode 上使用 copilot\n参考 这篇文章 ，秒解决~\ntrick (4) 在本地通过 py 代码控制服务器运行指令或者下载文件与文件夹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import paramiko from scp import SCPClient # 连接参数 hostname = \u0026#39;\u0026#39; # 云服务器 IP port = 22 # 一般是 22 username = \u0026#39;\u0026#39; # 云服务器用户名（如 root） # password = \u0026#39;your_password\u0026#39; # 云服务器密码 def control(command=None): # 创建 SSH 客户端 ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(hostname, port, username) # 控制服务器执行远程命令 try: stdin, stdout, stderr = ssh.exec_command(command) except Exception as e: print(f\u0026#34;命令执行失败: {e}\u0026#34;) out, err = stdout.read().decode(), stderr.read().decode() ssh.close() return out, err def download(): # 创建SCP客户端 ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(hostname, port, username) scp = SCPClient(ssh.get_transport()) # 下载文件与文件夹 try: # 想要下载的文件或者文件夹 下载的目录 scp.get(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;) scp.get(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, recursive=True) print(\u0026#34;文件下载成功\u0026#34;) except Exception as e: print(f\u0026#34;文件下载失败: {e}\u0026#34;) # 关闭SCP和SSH连接 scp.close() ssh.close() 虚拟机联网（小插曲） 当时试了很多方法也没有让服务器连接上网络，于是我决定去自己的虚拟机上先配置好深度学习的环境，然后把环境文件直接传输到服务器上。然后很逆天的发现，自己的虚拟机也连不上网络？然后有试了很多乱七八糟的方法，直到我看到了 这篇文章 ，直接还原默认设置，完美解决了！\n可以看到后面直接 ping baidu.com 正在顺利的进行着。\n","date":"2025-05-26T18:25:36+08:00","permalink":"https://mhw-mathcode.github.io/p/vscode-%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","title":"Vscode 远程连接实验室服务器训练模型"},{"content":"hello world! ","date":"2025-01-21T10:09:52+08:00","permalink":"https://mhw-mathcode.github.io/p/myfirstblog/","title":"MyFirstBlog"}]