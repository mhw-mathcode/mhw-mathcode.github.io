[{"content":"Easy Medium Hard [寻找两个正序数组的中位数] 题意：对于两个有序数组，使用 $O(log(m+n))$ 的时间复杂度找到中位数。\n做法：\n双指针 $O(n+m)$ 将两个数组分为两组，保证第一组的最大值小于第二组的最小值，即可得到中位数。如果 $nums1$ 中有 $i$ 个数在第一组，那么 $nums2$ 中就有 $j=(sum+1)/2-i$ 个数在第一组，因此第一组最大值为 $max(nums1[i], nums2[j])$。剩余元素为第二组，最小值为 $min(nums1[i + 1], nums2[j + 1])$ 。因为数组本身有序，所以只需要求 $ nums1[i] \u0026lt; nums2[j + 1]$ 并且 $nums2[j] \u0026lt; nums1[i + 1]$。一个细节是，我们在 $i\u0026ndash;,j++$ 双指针移动的过程中，如果遇到了 $nums1[i + 1] \u0026gt; nums2[j]$ 即结束循环，那其实一定有 $nums1[i] \u0026lt; nums2[j + 1]$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public: double findMedianSortedArrays(vector\u0026lt;int\u0026gt;\u0026amp; nums1, vector\u0026lt;int\u0026gt;\u0026amp; nums2) { if (nums1.size() \u0026gt; nums2.size()) swap(nums1, nums2); int sum = nums1.size() + nums2.size(); nums1.insert(nums1.begin(), INT_MIN); nums2.insert(nums2.begin(), INT_MIN); nums1.push_back(INT_MAX); nums2.push_back(INT_MAX); // nums1 中前 i 个数与 nums2 中前 j 个数组成第一组，最大值为 max(nums1[i], nums2[j]) // 剩余元素为第二组，最小值为 min(nums1[i + 1], nums2[j + 1]) int i = 0, j = (sum + 1) / 2; // 所以要求 nums1[i] \u0026lt; nums2[j + 1] 并且 nums2[j] \u0026lt; nums1[i + 1] // nums1[i + 1] \u0026gt; nums2[j] 的同时 nums1[i] \u0026lt; nums2[j + 1] while (nums1[i + 1] \u0026lt; nums2[j]) { i++, j--; } if (sum \u0026amp; 1) return max(nums1[i], nums2[j]); else return (max(nums1[i], nums2[j]) + min(nums1[i + 1], nums2[j + 1])) / 2.0; } }; 二分优化 $O(log(min(m, n)))$，如果可以理解上面的思路，那么其实就是在第一组中找到最小的 $i$ 使得 $nums1[i + 1] \u0026gt;= nums2[j]$ 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: double findMedianSortedArrays(vector\u0026lt;int\u0026gt;\u0026amp; nums1, vector\u0026lt;int\u0026gt;\u0026amp; nums2) { if (nums1.size() \u0026gt; nums2.size()) swap(nums1, nums2); int sum = nums1.size() + nums2.size(); nums1.insert(nums1.begin(), INT_MIN); nums2.insert(nums2.begin(), INT_MIN); nums1.push_back(INT_MAX); nums2.push_back(INT_MAX); int l = 0, r = nums1.size() - 1, ans = r; while (l \u0026lt;= r) { int i = (l + r) / 2; int j = (sum + 1) / 2 - i; if (nums1[i + 1] \u0026gt;= nums2[j]) r = i - 1, ans = min(ans, i); else l = i + 1; } int i = ans, j = (sum + 1) / 2 - i; if (sum \u0026amp; 1) return max(nums1[i], nums2[j]); else return (max(nums1[i], nums2[j]) + min(nums1[i + 1], nums2[j + 1])) / 2.0; } }; [缺失的第一个正数] 题意：使用 $O(n)$ 的时间复杂度与常数级的空间复杂度找到一个数组的 $mex$。\n","date":"2025-08-07T11:38:59+08:00","permalink":"https://mhw-mathcode.github.io/p/leetcode-hot100/","title":"LeetCode Hot100"},{"content":"过程 1. 数据分析 (1) 统计分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pandas as pd # 训练数据 data_train=pd.read_csv(\u0026#34;train.csv\u0026#34;) # 测试数据 data_test = pd.read_csv(\u0026#34;test.csv\u0026#34;) # 查看各列属性的数据量和缺失情况 print(data_train.info()) print(data_test.info()) # 查看各列属性的基本统计信息， print(data_train.describe()) print(data_test.describe()) 共 12 条属性，其中\nPassengerId（乘客ID），Name（姓名），Ticket（船票信息）作为乘客一般信息，不参与后续分析讨论； Survived（获救情况）为因变量； Pclass（乘客等级），Sex（性别），Embarked（登船港口）是明显的类别型数据，而 Age（年龄），SibSp（堂兄弟妹个数），Parch（父母与小孩的个数）则是隐性的类别型数据；Fare（票价）是数值型数据；Cabin（船舱）则为文本型数据； Age（年龄），Cabin（船舱）和Embarked（登船港口）信息存在缺失数据。 在测试集中，Age（年龄），Cabin（船舱）和 Fare（票价）信息存在缺失数据。\n(2) 属性分析 Pclass（乘客等级），Sex（性别），Embarked（登船港口），Age（年龄），Fare（票价）\n","date":"2025-08-03T08:36:36+08:00","permalink":"https://mhw-mathcode.github.io/p/kaggle-%E5%88%9D%E4%BD%93%E9%AA%8C-1--titanic/","title":"Kaggle 初体验 (1) -- Titanic"},{"content":"高数 线性代数 概率论 ","date":"2025-07-16T21:09:56+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E5%AD%A6%E5%90%88%E9%9B%86%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E6%A6%82%E7%8E%87%E8%AE%BA/","title":"学习笔记-数学合集（高等数学、线性代数、概率论）"},{"content":"1. 数据库系统概述 数据：描述事物的符号记录\n数据库：数据库是长期储存在计算机内、有组织的、可共享的大量数据的集合\nDBMS：数据库管理系统\n数据源 (DATA SOURCE)：远程数据库的别名 ODBC：开发工具访问数据库统一的平台 JDBC：Java 数据库连接 数据库系统：数据库系统是由数据库、数据库管理系统（及其应用开发工具）、应用程序和数据库管理员（DataBase Administrator, DBA）组成的存储、管理、处理和维护数据的系统\n数据独立性包括物理独立性和逻辑独立性（本质：希望不管怎么变，想不修改应用程序）\n物理独立性：指用户的应用程序与数据库中数据的物理存储是相互独立的 逻辑独立性：指用户的应用程序与数据库的逻辑结构是相互独立的 关系数据库系统采用关系模型作为数据的组织方式\n关系 (ralation)：一个关系对应通常说的一张表 元组 (tuple)：表中的一行即为一个元组 属性 (attribute)：表中的一列即为一个属性，给每一个属性起一个名称即为属性名 码, 键 (key)：表中的某个属性组，它可以唯一确定一个元组 域 (domain)：域是一组具有相同数据类型的值的集合。即某个属性的取值范围 分量：元组中的一个属性值 关系模式：对关系的描述，一般表示为 关系名 (属性1，属性2，···，属性n) 数据库系统结构： 模式 (schema)：模式也称逻辑模式，是数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。\n外模式 (external schema)：它是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述\n内模式 (internal schema)：一个数据库只有一个内模式。它是数据物理结构和存储方式的描述\n2. 关系数据库 候选码 (key)：某一属性组的值能唯一地标识一个元组，而其真子集不能，则称该属性组为候选码 (candidate key) 主码：若一个关系有多个候选码，则选定其中一个为主码 (primary key)，主码只可以有一个 主属性 (prime attribute)：候选码的各个属性称为（主属性） 非主属性：不包含在任何候选码中的属性 全码 (all key)：关系模式的所有属性是这个关系模式的候选码； 代理键 (id，序列号、序号，系统自动生成，自增长) 关系的完整性：\n实体完整性：若属性 A 是基本关系的主属性，则 A 不能取空值 引用完整性 关系代数：\n并、交、差 选择：选择是在关系 R 中选择满足给定条件的诸元组，记作 $δF(R) = {t|t∈R∩F(t)=\u0026lsquo;真\u0026rsquo;}$。 投影：关系 R 上的投影是从 R 中选择出若干属性列组成新的关系，记作 $∏A(R) = {t[A]|t∈R}$。 笛卡尔积 自然连接、等值连接 3. 关系数据标准语言SQL 数据定义：模式、表、索引 数据查询 数据更新：插入元组、插入子查询结果 修改数据 删除数据 视图：（投影 + 连接）\n视图能够简化用户的操作（简化操作） 视图使用户能以多种角度看待同一数据（多角度） 视图对重构数据库提供了一定程度的逻辑独立性（逻辑独立性） 视图能对机密数据提供安全保护（安全保护） 适当利用视图可以更清晰地表达查询（清晰表达） 索引：用于优化数据检索性能，支持快速查询、排序、分组和唯一性约束。 过程与函数：用于封装复杂的操作，提高代码复用性、效率和安全性，实现复杂的业务逻辑。\n6. 关系数据理论 属性闭包\n阿姆斯特朗公理：自反律、增广律、传递律\n等价的最少的函数依赖集：\n右边单一化：将每个函数依赖的右边分解为单个属性。 左边最小化：对于每个函数依赖 X→A，检查是否可以去掉 X 中的某些属性而不改变函数依赖的语义。 去除冗余依赖：检查函数依赖集中是否有冗余的依赖，即去掉某个依赖后，其他依赖是否可以推导出该依赖。 函数依赖集：它是一组函数依赖的集合，每个函数依赖表示一个属性集合能够唯一确定另一个属性集合。\n第一范式 (1NF)：数据表中每个字段的值必须具有原子性，也就是说数据表中每个字段的值为不可再次拆分的最小数据单元。\n第二范式 (2NF)：若 $R∈1NF$ ，所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分。\n第三范式 (3NF)：若 $R∈2NF$ ，数据表中的所有非主键字段不能依赖于其他非主键字段。\nBCNF：若 $R∈3NF$ ，并且每个决定方都是超键（超键是一个或一组属性，其值的组合能够唯一标识关系中的每一条记录）。\n","date":"2025-07-11T14:41:47+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/","title":"学习笔记-数据库原理"},{"content":"1. 计算机网络体系结构 计算机网络：一些互联的、自治的计算机系统的集合。功能：数据通信、资源共享。\nOSI：应用层，表示层，会话层，传输层，网络层，链路层，物理层。 TCP/IP：应用层，传输层，网络层，链路层，物理层。\n五层模型： 2. 物理层 三种通信方式：单工通信、半双工通信、全双工通信。\n同步通信：同步通信双方必须先建立同步，即双方的时钟要调整到同一个频率。收发双方不停的发送和接收连续的同步比特流。 异步通信：发送字符之间的时间间隔可以是任意的，但接收端必须时刻做好接收的准备。\n数据交换方式：\n电路交换：​ 整个报文段的比特流从源点连续的直达终点，专用物理连接线路，直到传输结束。包含建立连接、通信、释放连接三个阶段。 报文交换：将整个数据报发到相邻节点，全部存储下来，查找转发表，转发到下一个节点。 分组交换：把报文分组转发到相邻节点，查找转发表，转发到下一个节点。 3. 数据链路层 数据链路层在物理层提供服务的基础上向网络层提供服务，其最基本的服务是将来自网络层的数据可靠地传输到相邻节点的目标机网络层。其主要作用是加强物理层传输原始比特流的功能，将物理层提供的可能出错的物理连接改造成为逻辑上无差错的数据链路，使之对网络层表现为一条无差错的链路。\n数据链路层的五大功能：\n为网络层提供服务 链路管理：链路的建立、维持和释放 组帧：网络层交付下来的数据包叫做分组 ，数据链路层需要对分组封装成帧 ，帧是数据量链路层传输的基本单位； 流量控制：发送方、接收方之间传输速率和接收速率的差异较大，需要通过控制发送方来达到可靠数据传输； 差错控制：帧在传输过程中出现错误，有位错和帧错，差错控制负责发现错误，解决错误。 组帧：\n数据帧是数据链路层的基本传输单位，帧 = 帧首部 + IP数据报 + 帧尾部；数据链路层把帧（比特组）做为传输单位，在出错时只需重发出错的帧，而不必重发全部数据。 差错控制：\n传输中的差错一般都是由于噪声引起的； 差错类型： 位错：比特位出错； 帧错：帧丢失（重传）、帧重复和帧失序（帧序号）； 检错编码：奇偶校验码、循环冗余码（CRC） 纠错编码：海明码 流量控制：\n流量控制：控制发送速率，使接收方有足够的缓冲空间来接受每一个帧。通常用于解决发送较快、接收较慢而造成的传输错误。 数据链路层的流量控制手段：接收方收不下就不回复确认； 传输层的流量控制手段：接收方给发送方一个窗口公告。 流量控制的方法： 停止-等待协议：发送方每发一个帧就会停止发送，等待对方的确认信号（ACK），然后再发送下一个帧。 滑动窗口协议： 后退 N 帧协议（GBN） 选择重传协议（SR） 介质访问控制：\n介质访问控制：采用一些措施，使得两对结点之间的通信不会互相干扰。 CSMA 协议：先听再发。 CS：载波侦听/监听，每一个站在发送数据之前以及发送数据时都要检测一下总线上是否有其他计算机在发送数据； MA：多点接入，表示许多计算机以多点接入的方式连接在一根总线上。 CSMA/CD 协议：先听再说，边听边说。CD，碰撞检测。 CSMA/CA 协议：CA，避免碰撞。 截断二进制指数规避算法：发生碰撞后，什么时候重传？ ","date":"2025-07-04T21:04:41+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","title":"学习笔记-计算机网络"},{"content":"1. 线性表 数据结构三要素：逻辑结构、数据的运算、存储结构。\n线性表是具有相同数据类型的n(n\u0026gt;=0)个数据元素的有限序列。（逻辑结构）\n基本操作：创建、销毁、插入、删除、按值查找、按位查找。\n顺序表 = 线性表 + 顺序存储（静态分配 or 动态分配）\n链表 = 线性表 + 链式存储（单链表、双链表、循环单链表、循环双链表、静态链表）\n2. 栈和队列 栈是只允许在一端进行查入或者删除操作的线性表。\nn 个不同元素进栈，出栈元素不同排列的个数为 $\\frac{1}{n+1} C_{2n}^n$ （卡特兰数）。 顺序栈（共享栈）、链栈 队列是只能在队尾插入、在队首删除的线性表。\n顺序队列、循环队列、链式队列、双端队列 表达式求值：\n中缀转后缀： （手算）先确定各个运算符的顺序，然后按照 [左操作数 右操作数 运算符] 的方式组合。 （机算） 遇到操作数直接加入后缀表达式； 遇到界限符，遇到 ( 直接入栈，遇到 ) 则依次弹出栈内运算符并加入后缀表达式，直到弹出 ( 为止； 遇到运算符，依次弹出栈中优先级高于或等于当前运算符的所有运算符，并加入后缀表达式，若碰到 ( 或栈空则停止。之后再把当前运算符入栈。处理完所有字符后，将栈中剩余运算符依次弹出，并加入后缀表达式。 后缀表达式（逆波兰表达式）计算： 从左往右扫描后缀表达式； 扫描操作数入栈； 扫描操作符，弹出两个栈顶元素，计算结果入栈。 中缀表达式计算： 初始化两个栈，操作数栈和运算符栈； 若扫描到操作数，压入操作数栈； 若扫描到运算符或界限符，则按照“中级转后缀”相同的逻辑压入运算符栈（期间会弹出运算符，每当弹出一个运算符时，需弹出两个操作数栈的栈顶元素并执行运算，结果压回操作数栈）。 特殊矩阵压缩存储：对阵矩阵，三角矩阵，三对角矩阵，稀疏矩阵（三元组、十字链表法）。\n3. 串 串是由零个或多个字符组成的有限序列。\nKMP 算法（时间复杂度 $O(n+m)$）：\n当主串与模式串 $S$ 不匹配时，主串 $i$ 不回溯，模式串 $j=next[j]$ （模式串的 $next$ 数组）。 $next[j]$：由1~j-1个字符组成的串最长相等前后缀长度+1（取决于下标从 0 还是 1 开始）。 1 2 3 4 5 6 7 8 9 10 int i = 0, j = -1; while(i \u0026lt; T.size() - 1) { // T[i]表示后缀的单个字符, T[j]表示前缀的单个字符 if(j == -1 || T[i] == T[j]) { ++i, ++j; next[i] = j; } // 如果字符不相同，则j值回溯 else j = next[j]; } $nextval[j]$：$next$ 数组的优化 如果 $S[next[j]]==S[j]$ ，表明这其实是一次无效的比较，$nextval[j]=nextval[next[j]]$； 否则，$nextval[j]=next[j]$。 4. 树 概念与性质 结点的度：结点的分支数 树的度：树中各结点的度的最大值 结点数=总度数（边数）+1\nm叉树：可以所有结点的度都 $\u0026lt;= m$（结点的度最大为 $m$ ，可以为空树） 度为m的树：至少一个结点的度 $= m$（至少 $m+1$ 个结点）\n二叉树 二叉树是度为 2 的有序树（每个结点至多两个子树，左右子树不能颠倒）。\n满二叉树：不存在度为 1 的结点。 完全二叉树：在满二叉树的基础上，从最后一个结点开始去结点。 二叉排序树：左子树关键字均小于根节点的关键字，右子树关键字均大于根节点的关键字。 平衡二叉树（平衡二叉搜索树）：树上任意一个结点的左子树和右子树深度之差不超过 1 。 顺序存储（左儿子 $i2$ ，右儿子 $i2+1$ ）、链式存储\n性质：\n二叉树：叶子结点（度为 0 的结点）的数量比度为 2 的结点的数量多一个 具有 n 个结点的完全二叉树的高度为 $h = \\lceil \\log_2 (n + 1) \\rceil$ 或 $h = \\lfloor \\log_2 n \\rfloor + 1$ 。 遍历二叉树：\n先序遍历：根 \u0026ndash;\u0026gt; 左 \u0026ndash;\u0026gt; 右 中序遍历：左 \u0026ndash;\u0026gt; 根 \u0026ndash;\u0026gt; 右 后序遍历：左 \u0026ndash;\u0026gt; 右 \u0026ndash;\u0026gt; 根 层序遍历：$bfs$ 由遍历序列构造二叉树：\n前序+中序 后序+中序 层序+中序 线索二叉树：\n定义 1 2 3 4 5 6 // 左、右线索标志 typedef struct ThreadNode{ ElemType data; struct ThreadNode *lchild, *rchild; int lTag, rTag;\t// 0指向孩子；1指向线索 } 中序线索化（先序、后序类似） 应用：找中序后继（剩余同理） 在中序线索二叉树中，如果 $p \\rightarrow rTag==1$ ，右孩子指针被线索化了，那么直接得到中序后继； 若 $p \\rightarrow rTag==0$ ，有右孩子。就要找右子树得中序遍历最左边的结点。 树与森林 树的存储结构：\n双亲表示法（顺序存储） 孩子表示法（顺序存储+链式存储） 孩子兄弟表示法（顺序存储+链式存储，左指针指向儿子，右指针指向兄弟）（树、森林与二叉树的转换） 树的遍历：\n先根遍历：先访问根节点，再对每颗子树进行先根遍历；树的先根遍历序列和 对应的二叉树的先序序列相同。 后根遍历：先对每颗子树进行后根遍历，再访问根节点；树的后根遍历序列和对应的二叉树的中序序列相同。 森林的遍历：\n先序遍历 中序遍历 应用 二叉排序树（BST）：\n二叉排序树，又叫二叉查找树（Binary Search Tree），其左子树关键字均小于根节点的关键字，右子树关键字均大于根节点的关键字。 查找、插入、构造、删除（度为 2 的结点需要找前驱或者后继） 平衡二叉树（AVL）：\n平衡因子 = 左子树高 - 右子树高，任一结点平衡因子绝对值小于 1。 插入： 每次只需调整最小不平衡子树。 高度为 h 时最少有 $\\frac{h*(h-1)}{2}+1$ 个结点。 哈夫曼树：\n哈夫曼树：带权路径长度（WPL）最小的二叉树 结点的带权路径长度：从根结点到该结点的路径长度 * 权值 树的带权路径长度：所有叶结点的带权路径长度之和 构造： 选权值最小的两个结点； 在剩下的结点中挑一个最小的结点继续结合；或者挑两个结点先结合。 性质： 结点总数为 $2*n-1$ 不存在度为 1 的结点 哈夫曼树不唯一 哈夫曼编码： 固定长度编码：每个字符用相等长度的二进制位表示 可变长度编码：允许对不同字符用不等长的二进制位表示 前缀编码：没有一个编码是另一个编码的前缀 将字符频次作为字符结点权值，构造哈夫曼树，可得到哈夫曼编码，可用于数据压缩。 5. 图 概念 图 G 就是由点集 V 和边集 E 组成的。 无向图和有向图：有向边 \u0026lt;A, B\u0026gt; （弧尾，弧头），无向边 (A, B) 简单图：不存在重复的边；多重图：存在重复的边。 顶点的度 = 入度 + 出度 简单路径：顶点不重复出现；简单回路：除了头顶点和尾顶点，其余顶点里不出现重复的顶点。\n连通性：无向图中，$v — … — w$ （v，w之间是连通的）； 强连通性：有向图中，既有$v \\rightarrow … \\rightarrow w$ ，又有 $v \\leftarrow … \\leftarrow w$ ，（v，w 之间是强连通的）。 连通图：在无向图中，任意两个点连通； 强连通图：在有向图中，任意两个点强连通。最少有 n 条边（形成回路）。\n子图：部分点集+部分边集，每条边的两个点一定存在。 生成子图：子图包含原图的所有顶点，可以去掉一些边。 连通分量：在无向图中极大的连通子图 强连通分量：在有向图中极大的强连通子图\n生成树：对于连通图，包含图中所有顶点的极小连通子图 生成森林：对于非连通图，各连通分量的生成树组成了生成森林\n存储：\n邻接矩阵 邻接表 十字链表 邻接多重表 应用 最小生成树（MST）：\n$Prim$ ：$O(n^2)$，每次遍历所有结点找到加入 MST 的代价 $lowcast$ 最低的结点，然后用改结点更新所有结点的 $lowcast$。 $Kruskal$ ：$O(|E|log|E|)$，每次选择一条权值最小的且该边两端结点不连通的边。 最短路：\n单源最短路： $BFS$ ：无权图 $Dijkstra$ ：正权图，优先队列优化 $O(nlogn)$ 。 多源最短路： $Floyd$ ：无负权回路的图 有向无环图（Directed Acyclic Graph）描述表达式： 拓扑排序：\nAOV：顶点表示活动的网（Activity On Vertex Network） DAG 表示一个工程（工程就是活动的顺序序列集合） 关键路径：\nAOE：顶点表示事件，有向边表示活动，边的权值表示该活动的花销，称为 AOE 网（Activity On Edge Network）。 从源点到汇点的有向路径中，长度最大的路径成为关键路径，关键路径上的活动成为关键活动。 特性： 若关键活动耗时增加，则整个工程的工期将增长 缩短关键活动的时间，可以缩短整个工程的工期 当缩短到一定程度时，关键活动可能会变成非关键活动 可能有多条关键路径，只提高一条关键路径上的关键活动速度并不能缩短整个工程的工期，只有加快那些包括在所有关键路径上的关键活动才能达到缩短工期的目的。 6. 查找（B树、散列表） 概念 查找表：用于查找的数据集合。 静态查找表（仅查找）、动态查找表（插删操作）。 平均查找长度（ASL）：所有查找过程中关键字的比较次数的平均值。\n顺序查找、折半查找、分块查找（又叫索引顺序查找，块内无序、块间有序）\nB 树：\nB 树，又称多路平衡查找树，B 树中所有结点的孩子个数的最大值称为 B 树的阶，通常用 m 表示。一棵 m 阶 B 树或为空树，或为满足如下特性的 m 叉树： 树中每个结点至多有 m 棵子树，即至多含有 m-1 个关键字。\n若根结点不是终端结点，则至少有两棵子树。\n除根结点外的所有非叶结点至少有 $⌊m/2⌋$ 棵子树，即至少含有 $⌊m/2⌋-1$ 个关键字。\n所有非叶结点的结构如下，其中，$K_i（i = 1, 2, \u0026hellip;, n）$ 为结点的关键字，且满足 $K_1 \u0026lt; K_2 \u0026lt; \u0026hellip; \u0026lt; K_n$ ；$P_i（i = 0, 1, \u0026hellip;, n）$ 为指向子树根结点的指针，且指针 $P_{i-1}$ 所指子树中所有结点的关键字均小于 $K_i$，$P_i$ 所指子树中所有结点的关键字均大于 $K_i$ ，$n（⌊m/2⌋ - 1 ≤ n ≤ m - 1）$ 为结点中关键字的个数。\nn P₀ K₁ P₁ K₂ P₂ \u0026hellip; Kₙ Pₙ 所有的叶结点都出现在同一层次上，并且不带信息（可以视为外部结点或类似于折半查找判定树的查找失败结点，实际上这些结点不存在，指向这些结点的指针为空）。\n含 n 个关键字的 m 叉 B 树，高度满足以下不等式：$\\log_m(n + 1) \\leq h \\leq \\log_{\\lceil m/2 \\rceil} \\frac{n + 1}{2} + 1$\n插入：在插入 $key$ 后，若导致原结点关键字数超过上限，则从中间位置（$⌊m/2⌋$）将其中的关键字分为两部分，左部分包含的关键字放在原结点中，右部分包含的关键字放到新结点中，中间位置（$⌊m/2⌋$）的结点插入原结点的父结点。若此时导致其父结点的关键字个数也超过了上限，则继续进行这种分裂操作，直至这个过程传到根结点为止，进而导致 B 树高度增加 1。\n删除：\n删除非终端节点，找直接前驱或直接后继，转化为终端结点的删除； 删除终端结点 删除后结点关键字个数未低于下限，直接删除； 若低于下限： 兄弟够借：左兄弟富裕，用该结点前驱的前驱填补，右同理; 兄弟不够借：左右兄弟关键字均 $=\\lceil m/2 \\rceil - 1$，将左（右）兄弟结点及双亲结点中的关键字进行合并。 B+ 树：\nm 个关键字，对应 m 个分支，m 个子树，查找信息并不会停留在分支节点上，会一直查找到叶子结点。 散列表：\n也称哈希表，通过散列函数（哈希函数）将关键字与存储地址联系起来。若不同关键字映射到了同一个值，称为冲突（同义词）。冲突越少查找效率越高。 散列函数： 除留余数法：选不大于散列表长度的最大质数，为了让不同的关键字冲突尽可能少。 直接定址法：$H(key)=a*key+b$ 。 数字分析法：选取数码分布较为均匀的若干位作为散列地址。 平方取中法：取关键字平方值的中间几位作为散列地址。 处理冲突： 拉链法：将同义词存储在一个链表中。优化：同一链表中数据有序连接。 开放地址法：可存放新表项的空闲地址既向他的同义词表项开放，又向他的非同义词表项开放，即 $H_i=(H(key)+d_i) % m $。 线性探测法：$d_i=0,1,2,…,m-1$，即发生冲突时每次往后探测相邻的下一个单元是否为空。删除某个结点时需要做一个删除标记，否则会截断在他之后填入的关键字。 平方探测法：$d_i=0^2,1^2,-1^2,2^2,-2^2,…,k^2,-k^2$，相较于线性探测法更不易于产生聚集问题。 伪随机序列法：$d_i=某个伪随机序列$。 再散列法：准备多个散列函数，发生冲突就用下一个。 7. 排序 算法稳定性：关键字相同的两个元素，在排序之后相对位置不变。\n内部排序：数据都在内存中； 外部排序：数据无法全部放在内存中。\n排序算法 平均时间复杂度 最好情况 最坏情况 空间复杂度 排序方式 稳定性 冒泡排序 O(n²) O(n) O(n²) O(1) In-place 稳定 选择排序 O(n²) O(n²) O(n²) O(1) In-place 不稳定 插入排序 O(n²) O(n) O(n²) O(1) In-place 稳定 希尔排序 O(n log n) O(n log² n) O(n log² n) O(1) In-place 不稳定 归并排序 O(n log n) O(n log n) O(n log n) O(n) Out-place 稳定 快速排序 O(n log n) O(n log n) O(n²) O(log n) In-place 不稳定 堆排序 O(n log n) O(n log n) O(n log n) O(1) In-place 不稳定 计数排序 O(n + k) O(n + k) O(n + k) O(k) Out-place 稳定 桶排序 O(n + k) O(n + k) O(n²) O(n + k) Out-place 稳定 基数排序 O(n × k) O(n × k) O(n × k) O(n + k) Out-place 稳定 插入排序：\n直接插入排序：每次将一个待排元素按其关键字大小插入到前面已经排好的子序列中。 折半插入排序，先用折半查找找到应该插入的位置，再移动元素。 希尔排序：先将待排序表分割为若干形如 $L[i,i+d,i+2d,…,i+kd]$ 的特殊子表，对各个子表分别进行直接插入排序。缩小增量 $d$ ，重复上述过程直到 $d=1$ 为止。\n交换排序：\n冒泡排序\n快速排序：在待排序表 $L[1\u0026hellip;n]$ 中任取一个元素 $pivot$ 作为枢纽（或基准，通常取首元素），通过一趟排序将待排序表划分为独立的两部分 $L[1\u0026hellip;k-1]$ 和 $L[k+1\u0026hellip;n]$，使得 $L[1\u0026hellip;k-1]$ 中的所有元素小于 $pivot$，$L[k+1\u0026hellip;n]$ 中的所有元素大于等于 $pivot$，则 $pivot$ 放在了其最终位置 $L(k)$ 上，这个过程称为一次“划分”。然后分别递归地对两个子表重复上述过程，直至每部分内只有一个元素或空为止，即所有元素放在了其最终位置上。\n选好基准，设置好 $low=1$、$high=n$ 指针； 因为设置首元素为枢轴元素，所以位置 0 为空，故 $low$ 所指向的位置 0 空，$high$ 先向左遍历； 若 $high$ 指针指向元素小于基准元素，则把该元素放到 $low$ 指向的空位置，此时 $high$ 指向的位置变为空，则开始向右遍历； 直到 $low$、$high$ 指针相遇，该轮快排结束。 将这个过程组织为二叉树，二叉树的层数就是递归调用的层数。\n选择排序：\n简单选择排序：每一次遍历选出最小的元素加入有序子序列。 堆排序： 堆：顺序存储的完全二叉树。结点i的左孩子是 $2i$；右孩子是 $2i+1$；父节点是 $\\frac{i}{2}$。编号 $\u0026lt;=\\frac{n}{2}$ 的结点都是分支结点 建堆：编号 $\u0026lt;=\\frac{n}{2}$ 的所有结点依次下坠调整，若不满足当前结点小于左右儿子，则将当前结点与更大的一个儿子交换。 排序：将堆顶元素与堆底元素交换，重新进行下坠操作，使其回复大根堆特性，重复 $n-1$ 趟。 归并排序：\n归并：把两个或多个子序列合并为一个。 归并排序： 若 $low \u0026lt; high$，则将序列分从中间 $mid=(low+high)/2$ 分开 对左半部分 $[low, mid]$ 递归地进行归并排序 对右半部分 $[mid+1,high]$ 递归地进行归并排序 将左右两个有序子序列 $Merge$ 为一个 基数排序：\n将整个关键字拆分为 d 位(或“组”) 按照各个 关键字位 权重递增的次序(如:个、十、百)，做 d 趟“分配”和“收集”若当前处理的 关键字位 可能取得 r 个值，则需要建立 r 个队列 分配：顺序扫描各个元素，根据当前处理的关键字位，将元素插入相应队列。一趟分配耗时 $O(n)$ 收集：把各个队列中的结点依次出队并链接。一趟收集耗时 $O(r)$ 外部排序：\n外部元素太多，无法一次全部读入内存进行排序，采用归并排序的思想和方法。外存中的数据读入内存 → 在内存中排序 → 数据写入外存，总时间开销 = 内部排序所需时间 + 内部归并所需时间 + 外部读写所需时间\n优化：多路归并\n1.对于 r 个初始归并段进行 k 路归并，需要归并趟数 = $log_k r$（向上取整，归并树高度） 2.提升外部排序的速度、减少读写磁盘的速度的方法：提高 k 值，降低 r 值。 提高 k 值：增加归并段长度。但是，提高 k 有负面影响：（1）需要的缓存空间升高（ k 路归并需 k 个缓冲区）；（2）内部归并的所需时间提高（选出最小关键字需要进行 k-1 次比较）。\n败者树：败者树可视为一棵完全二叉树(多了一个头头)。k 个叶结点分别对应 k 个归并段中当前参加比较的元素，非叶子结点用来记忆左右子树中的“失败者”，而让胜者往上继续进行比较，一直到根结点。使用多路平衡归并可减少归并趟数，构造败者树可以使关键字对比次数减少到 $log_2 k$。 置换-选择排序 设初始待排文件为 FI ，初始归并段输出文件为 FO ，内存工作区为 WA ，FO 和 WA 的初始状态为空，WA 可容纳 w 个记录。置换-选择算法的步骤如下：\n1）从 FI 输入 w 个记录到工作区 WA。 2）从 WA 中选出其中关键字取最小值的记录，记为 MINIMAX 记录。 3）将 MINIMAX 记录输出到 FO 中去。 4）若 FI 不空，则从 FI 输入下一个记录到 WA 中。 5）从 WA 中所有关键字比 MINIMAX 记录的关键字大的记录中选出最小关键字记录，作为新的 MINIMAX 记录。 6）重复3）～5），直至在 WA 中选不出新的 MINIMAX 记录为止，由此得到一个初始归并段，输出一个归并段的结束标志到 FO 中去。 7）重复2）～6），直至 WA 为空。由此得到全部初始归并段。 归并树 ","date":"2025-06-22T11:50:20+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"学习笔记-数据结构"},{"content":"开个坑，主要是记录后续复习操作系统的一些笔记与完成 6.S081 课程的 lab 。\n1. 概述 操作系统的定义：操作系统是一组控制和管理计算机软硬件资源，合理地组织多道程序的运行，方便用户使用的程序的集合。\n操作系统是系统资源的管理者（处理机管理，存储器管理，文件管理，设备管理）； 操作系统要向上提供方便易用的服务（GUI 接口，联机命令接口 = 交互式命令接口， 脱机命令接口 = 批处理命令接口，程序接口）； 操作系统是最接近硬件的一层软件。 简述操作系统的基本特征：\n并发：指两个或多个事件在同一时间间隔内发生。宏观上同时发生，微观上交替发生。（并发 vs 并行） 共享：系统中的资源可被多个并发执行的进程共同使用。并发性与共享性互为存在条件。 虚拟：将物理实体映射成若干个逻辑设备。没有并发性，实现虚拟性就没有意义。（虚拟处理器技术：时分复用技术；虚拟存储器技术：空分复用技术） 异步：多道程序下，由于资源有限，进程的执行是走走停停，以不可预知的速度向前推进。只有系统拥有并发性，才有可能导致异步性。 操作系统的运行机制：\n内核程序 vs 应用程序 CPU 执行的程序分为两种：操作系统内核程序、用户程序； 内核是操作系统最基础、核心的那部分；\n特权指令 vs 非特权指令 内核程序，在计算机中的地位充当管理程序 ，所以可以执行特权指令； 用户程序，在计算机中的地位充当被管理程序 ，出于安全考虑只能执行非特权指令；\n内核态 vs 用户态 【问题】CPU 可以区分特权指令和非特权指令 ，但是 CPU 无法识别正在执行的指令是应用程序的指令，还是内核程序的指令。 答：为了让 CPU 能够区分应用程序和内核程序 ，CPU会被划分为两种状态：内核态和用户态。\n内核态 - 内核程序 - 可以执行特权指令； 用户态 - 应用程序 - 只能执行非特权指令；\nCPU使用程序状态字寄存器（PSW） 实现对CPU状态的标记。\n如何变态 ？ 内核态转向用户态：执行一条特权指令，修改 PSW 的标志位为用户态； 用户态转向内核态：由中断引发，硬件自动完成变态过程；\n中断与异常：\n中断的作用：让操作系统内核强行夺回 CPU 控制权；使 CPU 从用户态变为内核态。（没有中断就无法实现并发） 分类： 中断机制的基本原理：不同的中断信号，需要查询“中断信号表”来找到不同的中断处理程序来处理。 系统调用：是操作系统提供给用户程序使用的接口，可以理解为一种特殊函数，应用程序可以通过系统调用来请求获得操作系统内核的服务。凡是与共享资源有关的操作，都必须通过系统调用的方式向操作系统内核提出服务请求。\n过程： 一个应用程序运行在用户态，那么它的指令会CPU被一条条执行； 当他想发出系统调用的时候，他需要传参指令给CPU的寄存器传入某个参数，这个参数指明要进行哪种系统调用；传参指令可能多条，主要看需要的系统调用要求几个参数； 当参数都传入寄存器之后，用户程序就会执行陷入指令，这个陷入指令得到执行会引发一个内中断； CPU 检测到内部中断，发现这个内中断是由 trap 指令引起的，就会暂停处理应用程序，转入相应的中断处理程序； CPU 转为内核态，处理系统调用入口程序，根据参数判断需要哪种系统调用；对应系统调用的处理程序根据传入的其它参数，看看用户程序需要哪些具体服务； 系统调用处理完，CPU切换为用户态，继续之前的用户程序。 操作系统的体系结构：\n操作系统的内核 大内核与微内核 分层结构：每一层只能调用更低、相邻的那一层提供的功能接口； 模块化：将操作系统分成多个模块，各模块之间协调工作； 外核：用户直接使用硬件资源。 开机过程：\n操作系统要启动运行，操作系统的数据就需要被放进主存里面；计算机的主存，由 RAM 和 ROM 组成；ROM 存储芯片里存储 BIOS（基本输入输出系统），BIOS 里包含 ROM 引导程序。 CPU 从一个特定主存地址开始，取指令，执行 ROM 中的引导程序（先进行硬件自检，再开机）； ROM 引导程序指示 CPU 将磁盘的第一块\u0026ndash;主引导记录读入内存，执行磁盘引导程序，扫描分区表； 磁盘引导程序会根据分区表去找到C盘的位置，之后 CPU 读入 C 盘的引导记录 PBR。PBR 本身也是一种程序，CPU 就执行 PBR 里的程序，PBR 程序的主要作用就是找到启动管理器。启动管理器是在根目录里的一个程序，找到启动管理器，CPU 运行它，就开始了操作系统初始化的一系列操作。 2. 进程与线程 2.1 进程与线程 进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位；\n​进程实体包括：PCB + 程序段 + 数据段 三部分；\nPCB：进程描述信息（PID，UID）、进程控制和管理信息、资源分配信息； 程序段：程序的代码； 数据段：运行过程中出生的各种数据。 进程的特征：\n动态性：进程是程序的一次执行过程，是动态地产生、变化和消亡的； 并发性：内存中有多个进程实体，各进程可并发执行； 独立性：进程是能独立运行、独立获得资源、独立接受调度的基本单位； 异步性：各进程按各自独立的、不可预知的速度向前推进，操作系统要提供“进程同步机制\u0026quot;来解决异步问题； 结构性：每个进程都会配置一个PCB。 进程的状态与转换： 进程的组织方式：\n链接方式：将同一状态的 PCB 链接成一个链表； 索引方式：将同一状态的进程组织在一个索引表中，索引表的表项指向相应的 PCB。 进程的控制：\n进程的创建 申请空白 PCB； 为新进程分配所需资源； 初始化 PCB； 将 PCB 插入就绪队列； 进程的终止 从 PCB 集中中找到终止进程的 PCB； 若进程正在运行，立刻剥夺 CPU， 将 CPU 分配给其它进程； 进程的阻塞 找到要阻塞的进程对应的 PCB； 保护进程运行现场，将 PCB 状态信息设置为 阻塞态，暂时停止进程执行； 将 PCB 插入相应事件的等待队列； 进程的唤醒 在事件等待队列中找到PCB； 将 PCB 从等待队列移除，设置进程为就绪态； 将 PCB 插入就绪队列，等待被调度； 进程状态的切换 将运行环境信息存入 PCB； PCB 移入相应队列； 选择另一个进程执行，并更新其 PCB； 根据 PCB 回复新进程所需的运行环境； 进程的通信：\n共享存储：相互通信的进程互斥地共享某些数据结构或存储区，进程之间能够通过这些空间进行通信； 消息传递：进程之间的数据交换以格式化的信息为单位，将通信的数据封装在信息中，并利用操作系统提供的一组通信命令（原语），在进程间进行信息传递，完成进程间的数据交换； 直接通信方式：送货上门 间接通信方式：快递到驿站 管道通信 线程：减少程序在并发执行时所付出的时间开销，提高 OS 的并发性能；引入线程后，进程是资源分配的基本单位，线程是调度的基本单位。线程的实现方式包括用户级线程与内核级线程。TCB 线程控制块；TID 线程标识符。\n2.2 处理机调度 处理机调度：进程数 \u0026gt; 处理机个数，需要对处理机进行分配。\n三层调度： 调度算法的评价指标： 进程调度：就是按照某种算法从就绪队列中选择一个进程为其分配处理机。进程在操作系统内核程序临界区中不能进行程序调度与切换的情况（普通临界区可以）。\n调度算法：\n先来先服务（FCFS）：按照进程到达的先后顺序； 短作业优先（SJF）：服务时间最短的进程优先；抢占式版本：最短剩余时间优先算法 SRTN； 高响应优先（HRRN）：响应比 = (等待时间 + 要求服务时间) / 要求服务时间； 时间片轮转调度算法（RR）：根据进程到达就绪队列的顺序，轮流调度； 优先级调度算法：调度时选择优先级最高的； 多级反馈队列调度算法：设置多级就绪队列；各队列按照 FCFS + 时间片，时间片结束进入下一级队列队尾；只有第 k 级队列为空时，才会为 k+1 级分配时间片。 同步与互斥：\n同步：并发进程为完成同一任务所进行的工作顺序协调； 互斥：并发进程为竞争临界资源所进行的资源分时占用。 进程互斥遵循的原则： 空闲让进：临界区空闲时，应允许一个进程访问 忙则等待：临界区正在被访问时，其他试图访问的进程需要等待 有限等待：要在有限时间内进入临界区，保证不会饥饿 让权等待：进不了临界区的进程，要释放处理机，防止忙等 互斥锁：\n互斥锁填充采用硬件来实现获得锁、释放锁的原子性；每个互斥锁有一个布尔变量 available ，表示锁释放可用； 特点：忙等，违反了让权等待；等待代价较低。 信号量：\n信号量机制的概念：把系统中的资源抽象化为变量，信号量记录着资源数；提供一对原语 wait(S) 和 signal(S) 来保证对信号量 S 操作的原子性（wait、signal 操作常被成为 P、V 原语）。 类别： 整型信号量：用一个整数型变量作为信号量，用来表示系统中某种资源的数量。但是不满足让权等待原则。 记录型信号量 使用： 实现进程互斥； 实现进程同步； 实现前驱关系。 经典同步问题：\n生产者-消费者问题 读者-写者问题 哲学家进餐问题 死锁：\n死锁：各进程互相等待对方的资源，导致各进程都阻塞（资源永远不会释放）。 特点：至少两个或两个以上的进程同时死锁；处于阻塞态。 区分\n饥饿：长期得不到想要的资源（资源会释放，但不会被分配） 特点：一个进程也可能出现饥饿现象；可能阻塞态（等 IO），也可能就绪态 （等 CPU）。 死循环：死锁和饥饿是由资源的不合理分配导致，死循环是由代码逻辑错误导致。 形成的必要条件： 互斥条件：只有对互斥资源的争抢才会导致死锁。 不可剥夺条件：进程保持的资源只能主动释放，不可强行剥夺。 请求和保持条件：保证着某些资源不放的同时，请求别的资源。 循环等待条件：存在一种进程资源的循环等待链；循环等待未必死锁，死锁必定循环等待。 死锁预防：破坏死锁的四个必要条件（完全没有死锁的可能）。 死锁避免：避免系统进入不安全状态（有可能死锁，避免陷入死锁）。 银行家算法 ：在资源分配之前，先判断此次分配是否会导致系统进入 不安全状态 ，再决定是否分配资源。 3. 内存管理 内存的一大作用是缓和 CPU 与硬盘之间的速度矛盾。\n内存管理任务：\n内存空间的分配与回收 内存空间的扩充：从逻辑上对内存空间进行扩充 地址转换：逻辑地址与物理地址的转换 存储保护：保证各进程在各自存储空间内运行，互不干扰。 进程运行的基本原理：\n创建进程首先要将程序和数据装入内存； 将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤： 编译：由编译程序将用户源代码编译成若干个模块（机器指令）； 链接：由链接程序将编译后形成的一组目标模块及它们所需的库函数链接在一起，形成一个完整的装入模块； 装入：由装入程序将装入模块装入内存运行。 三种\n","date":"2025-06-17T18:27:49+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"学习笔记-操作系统"},{"content":"0 算法速览 监督学习算法：\n线性回归（Linear Regression）：用于回归任务，预测连续的数值。 逻辑回归（Logistic Regression）：用于二分类任务，预测类别。 决策树（Decision Tree）：基于树状结构进行决策的分类或回归方法。 支持向量机（SVM）：用于分类任务，构建超平面进行分类。 K近邻算法 集成学习 无监督学习算法：\nK-means 聚类：通过聚类中心将数据分组。 主成分分析（PCA）：用于降维，提取数据的主成分。 0.1 线性回归 线性回归 (Linear Regression) 是一种用于预测连续值的最基本的机器学习算法，它假设目标变量 y 和特征变量 x 之间存在线性关系，并试图找到一条最佳拟合直线来描述这种关系。\n常用的误差函数是均方误差 (MSE) : $MSE = 1/n * Σ(y_i - y_{pred_i})^2$\n求解方法-最小二乘法 最小二乘法的目标是最小化残差平方和（RSS），其公式为：$RSS = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$\n得到最佳的 $w, b$\n$$ \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i^2 \u0026 \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i \u0026 n \\end{bmatrix}^{-1} \\begin{bmatrix} \\sum_{i=1}^n x_i y_i \\\\ \\sum_{i=1}^n y_i \\end{bmatrix} $$求解方法-梯度下降法 梯度下降法的目标是最小化损失函数 $J(w,b)$ 。对于线性回归问题，通常使用均方误差（MSE）作为损失函数：\n$$ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$参数更新：\n$$ w := w - \\alpha \\frac{\\partial J}{\\partial w} \\quad b := b - \\alpha \\frac{\\partial J}{\\partial b} $$梯度下降法的步骤\n初始化参数：初始化 w 和 b 的值（通常设为 0 或随机值）。 计算损失函数：计算当前参数下的损失函数值 $J(w,b)$ 。 计算梯度：计算损失函数对 w 和 b 的偏导数。 更新参数：根据梯度更新 w 和 b。 重复迭代：重复步骤 2 到 4，直到损失函数收敛或达到最大迭代次数。 0.2 逻辑回归 逻辑回归（Logistic Regression）是一种广泛应用于分类问题的统计学习方法，尽管名字中带有\u0026quot;回归\u0026quot;，但它实际上是一种用于二分类或多分类问题的算法。\n逻辑回归通过使用逻辑函数（也称为 Sigmoid 函数）将线性回归的输出映射到 0 和 1 之间，从而预测某个事件发生的概率。建立模型：\n\\[ p(y = 1|X) = \\sigma(w^T X + b) \\]其中：\n\\( X \\) 是输入特征（可以是多个特征组成的向量）。 \\( w \\) 是权重向量。 \\( b \\) 是偏置项。 \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) 是Sigmoid函数。Sigmoid函数将模型的输出值 \\((w^T X + b)\\) 映射到0到1之间，因此它可以看作是属于类别1的概率。注意 $\\sigma\u0026rsquo;(z) = \\sigma(z)(1 - \\sigma(z))$ 。 使用对数损失函数\n\\[ L(\\theta) = \\begin{cases} -\\log(p), \u0026 \\text{if } y = 1 \\\\ -\\log(1-p), \u0026 \\text{if } y = 0 \\end{cases} \\]合并得到单个样品的损失函数\n\\[ L(\\theta) = -ylog(p)-(1-y)log(1-p) \\]因此总体损失函数（也就是交叉熵损失函数）\n\\[ L(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(p^{(i)}) + (1 - y^{(i)}) \\log(1 - p^{(i)}) \\right] \\]求解方法-梯度下降法 对 \\(w\\) 的梯度： \\[ \\frac{\\partial J(w, b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\]对 \\(b\\) 的梯度： \\[ \\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\]\n0.3 决策树 决策树（Decision Tree），它是一种以树形数据结构来展示决策规则和分类结果的模型，作为一种归纳学习算法，其重点是将看似无序、杂乱的已知数据，通过某种技术手段将它们转化成可以预测未知数据的树状模型，每一条从根结点（对最终分类结果贡献最大的属性）到叶子结点（最终分类结果）的路径都代表一条决策的规则。\n在这个过程中，寻找最优划分属性是决策树过程中的重点，那么应该如何求解呢？\n求解方法-信息增益 首先引入信息熵的概念\n信息熵：描述随机变量的不确定性（也就是混乱程度）。 假设某随机变量的概率分布为：$P(X=x_i) = p_i, \\quad i = 1, 2, \\ldots, n$ ，则它的信息熵计算公式为：$H(X) = - \\sum_{i=1}^{n} p_i \\log p_i$\n在决策树中，信息熵\n\\[ H(D) = - \\sum_{k=1}^{K} \\frac{|D_k|}{|D|} \\log \\frac{|D_k|}{|D|} \\]其中：\n\\( D \\)：整个数据集 \\( D_k \\)：第 \\( k \\) 个类的样本子集 \\( \\frac{|D_k|}{|D|} \\)：第 \\( k \\) 类的概率 条件熵\n\\[ H(D|A) = \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} H(D_i) \\]\\( H(D_i) \\) 是每个子集 \\( D_i \\) 的信息熵\n\\[ H(D_i) = - \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|} \\log \\frac{|D_{ik}|}{|D_i|} \\]其中：\n\\( A \\)：某个属性 \\( D_i \\)：属性 \\( A \\) 的第 \\( i \\) 个取值所对应的数据子集 \\( D_{ik} \\)：在第 \\( i \\) 个子集中属于第 \\( k \\) 类的样本数 因此有\n\\[ H(D|A) = - \\sum_{i=1}^{n} \\frac{|D_i|}{|D|} \\sum_{k=1}^{K} \\frac{|D_{ik}|}{|D_i|} \\log \\frac{|D_{ik}|}{|D_i|} \\]特征 \\( A \\) 对训练数据集 \\( D \\) 的信息增益 \\( gain(D, A) \\) 定义为集合 \\( D \\) 的信息熵 \\( H(D) \\) 与特征 \\( A \\) 给定条件下 \\( D \\) 的信息条件熵 \\( H(D|A) \\) 之差，即公式为：\n\\[ gain(D, A) = H(D) - H(D|A) \\]信息增益表示得知特征 \\( X \\) 的信息而使得类 \\( Y \\) 的信息的不确定性减少的程度，因此信息增益最大的属性就是最优划分属性，标志性算法 $ID3$ 。\n求解方法-增益比 信息增益虽然在理论上可以找到最优的划分属性，但在某些情况下会存在问题。信息增益比较偏好可取值较多的属性。因此为了矫正信息增益偏好的问题，使算法不偏向可取值较多的属性，引申出了增益比的思想。\n\\[ Gain\\_ratio(D, A) = \\frac{Gain(D, A)}{H(A)} \\]可以看出，增益比就是信息增益除以属性 $A$ 的信息熵，当属性 $A$ 可取值增多的时候，$H(A)$ 一般也增大，因此在一定程度上能抑制信息增益偏好取值多的属性的特点，但是增益比偏好取值较少的属性。\n算法 $C4.5$ 是算法 $ID3$ 的改进版，它使用了信息增益和增益比两种选择算法，先选出信息增益高于平均水平的属性，然后再在这些属性中选择增益比最高的，作为最优划分属性。这样综合了信息增益和增益比的优点，可以取得较好的效果。\n求解方法-基尼指数 基尼指数是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。这个规律与信息熵的相同。\n\\[ Gini(D) = \\sum_{k=1}^{n} \\sum_{k' \\neq k} p_k p_{k'} = 1 - \\sum_{k=1}^{n} p_k^2 \\]使用基尼指数来选择最优划分属性也是对比不同属性划分后基尼指数的差值，选择使样本集基尼指数减小最多的属性。\n\\[ Gain(D, a) = Gini(D) - \\sum_{i=1}^{n} \\frac{|D^i|}{|D|} Gini(D^i) \\]著名的 $CART$ 决策树就是使用基尼指数来作为划分准则， $CART$ 决策树与 $ID3$ 和 $C4.5$ 的区别：\n划分准则不同，CART决策树使用基尼指数， $ID3$ 和 $C4.5$ 使用信息熵。\n$ID3$ 和 $C4.5$ 划分时，一个节点可以划分为多个子结点，子结点数量根据属性可取值的数量决定。而 $CART$ 决策树是严格的二叉树结构，就是说 $1$ 个节点最多划分为 $2$ 子结点。\n0.4 支持向量机 支持向量机（Support Vector Machine，简称 SVM）是一种监督学习算法，主要用于分类和回归问题。\nSVM 的核心思想是找到一个最优的超平面，将不同类别的数据分开。这个超平面不仅要能够正确分类数据，还要使得两个类别之间的间隔（margin）最大化。\n支持向量：支持向量是离超平面最近的样本点。这些支持向量对于定义超平面至关重要。支持向量机通过最大化支持向量到超平面的距离（即最大化间隔）来选择最佳的超平面。\n当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机；\n当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机；\n当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机。\n求解方法-间隔最大化和支持向量 好难……\n0.5 K近邻算法 K 近邻算法（K-Nearest Neighbors，简称 KNN）是一种简单且常用的分类和回归算法。\nK 近邻算法属于监督学习的一种，核心思想是通过计算待分类样本与训练集中各个样本的距离，找到距离最近的 K 个样本，然后根据这 K 个样本的类别或值来预测待分类样本的类别或值。\n求解方法 基本步骤：\n计算距离：计算待分类样本与训练集中每个样本的距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。\n选择 K 个最近邻：根据计算出的距离，选择距离最近的 K 个样本。\n投票或平均：对于分类问题，K 个最近邻中出现次数最多的类别即为待分类样本的类别；对于回归问题，K 个最近邻的值的平均值即为待分类样本的值。\n0.6 集成学习 0.7 K-means 聚类 K-means 聚类是一种常用的基于距离的聚类算法，旨在将数据集划分为 K 个簇。算法的目标是最小化簇内的点到簇中心的距离总和。\n求解方法 基本步骤：\n选择 $K$ 值：设定簇的数量 。\n初始化簇中心：随机选择 $K$ 个数据点作为初始簇中心（centroids）。\n分配步骤（Assignment Step）：对于数据集中的每个点，将它分配到最近的簇中心对应的簇。这里的“距离”通常使用欧氏距离（Euclidean distance）。\n更新步骤（Update Step）：根据当前的簇分配，重新计算每个簇的中心，即计算簇内所有点的均值作为新的簇中心。\n重复 3 和 4 步：不断重复分配和更新步骤，直到簇中心不再发生变化（收敛）或达到指定的最大迭代次数。\n确定最佳的簇数 $K$ 是 $K-means$ 聚类中的一个难点。聚类的目标是使得每个样本点到距离其最近的聚类中心的总误差平方和（也即聚类的代价函数，记作 $SSE$ ）尽可能小。\n空间中数据对象与聚类中心间的欧式距离计算公式为：\n\\[ d(x, C_i) = \\sqrt{\\sum_{j=1}^{m} (x_j - C_{ij})^2} \\]其中：\n\\( x \\) 为数据对象， \\( C_i \\) 为第 \\( i \\) 个聚类中心， \\( m \\) 为数据对象的维度， \\( x_j \\)，\\( C_{ij} \\) 为 \\( x \\) 和 \\( C_i \\) 的第 \\( j \\) 个属性值。 整个数据集的误差平方和 SSE 计算公式为：\n\\[ SSE = \\sum_{i=1}^{k} \\sum_{x \\in C_i} |d(x, C_i)|^2 \\]其中：\nSSE 的大小表示聚类结果的好坏， \\( k \\) 为簇的个数。 理论上随着 $K$ 的增加， $SSE$ 会单调递减，当 $K$ 超过某一个数后，每个类簇的聚合程度不再获得显著提升，此时我们就可以认为已找到最佳 $K$ 的取值（肘部法）。\nK-means++ K-means++ 是一种改进的初始化方法，可以帮助选择更合理的初始中心，优先选择“距离最远”的点作为初始质心，减少陷入局部最优的风险。\n基本步骤：\n从数据集 $\\mathcal{X}$ 中随机（均匀分布）选取一个样本点作为第一个初始聚类中心;\n接着计算每个样本与当前已有聚类中心之间的最短距离，用 $D(x)$ 表示；然后计算每个样本点被选为下一个聚类中心的概率 $P(x) = \\frac{D(x)^2}{\\sum_{x \\in \\mathcal{X}} D(x)^2}$，最后选择最大概率值（或者概率分布）所对应的样本点作为下一个簇中心；\n重复步骤 2，直到选择 $K$ 个聚类中心\n优势：避免随机初始化，加快收敛速度，聚类结果更加稳定。\n0.8 主成分分析 主成分分析（PCA）是一种无监督学习方法，旨在通过线性变换将原始的高维数据映射到一个低维空间，同时尽可能保留数据的方差（即信息量）。简单来说，PCA 的目标是找到一组新的坐标轴（称为主成分），这些坐标轴能够捕捉数据中最大的变异性，并用更少的维度来近似表示原始数据。\n求解方法 数据中心化：首先将数据中心化，即让每个特征的均值变为 0。 计算协方差矩阵： \\[ \\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\]协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立。\n特征值分解：对协方差矩阵进行特征值分解，得到主成分的方向（特征向量）和重要性（特征值）。令\\( A \\) 是协方差矩阵，\\( \\lambda \\) 是特征值，\\( I \\) 是单位矩阵，求解 \\[ \\det(A - \\lambda I) = 0 \\] 得到特征值 \\( \\lambda \\) 以后代入 $(A - \\lambda I)v_1 = 0$ 解得特征向量 $v_1$ 。\n排序和选择主成分：将特征值从大到小排序，特征值最大的为第一个主成分，捕捉了数据中最大的变化，也就是数据分布中最显著的变化方向。第二个主成分与第一个主成分正交（相互垂直），且在正交约束下方差次大的方向。后续主成分：依此类推，每个主成分都与前面的主成分正交，并按特征值大小递减排列。\n投影数据：将中心化后的数据投影到第一个主成分上，得到降维后的结果。\n一些问题 为什么要计算协方差矩阵？ PCA 的目标是找到一组新的坐标轴（称为主成分），使得数据在这些轴上的投影方差最大化，同时这些轴相互正交（不相关）。协方差矩阵正好量化了数据中的变异性和变量间的相关性，我们可以了解到哪些变量变化较大，哪些变量之间存在较强的关联，为找到这样的轴提供了基础。\n为什么要进行特征值分解？ 特征值表示每个特征向量方向上的方差大小。特征值越大，说明该方向捕捉的变异性越多。通过特征值分解，我们可以将原始数据投影到这些特征向量上，从而实现降维，同时尽可能保留数据的信息。\n1 引言 1.1 什么是机器学习 一个好的学习问题定义如下：一个程序被认为能从经验 E 中学习，解决任务 T ，达到性能度量值 P ，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n目前存在几种不同类型的学习算法，其中主要的两种类型被我们称之为：监督学习和无监督学习。\n1.2 监督学习 监督学习：给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。\n回归 (regression) 问题：推测出这一系列连续值属性。\n分类 (classification) 问题：推测出离散的输出值。\n1.3 无监督学习 无监督学习：没有标签的数据，模型通过探索数据中的结构或模式来进行学习。\n聚类算法：将数据集划分成两个不同的簇。\n鸡尾酒算法：分离两种声音。（一个具体实例，仅仅只需要一行代码实现）\n1 [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x\u0026#39;); 2 单变量线性回归 (Linear Regression with One Variable) ","date":"2025-06-16T19:29:56+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","title":"学习笔记-机器学习"},{"content":"DAY1 因为队友有考试，所以我们集体在周六早晨赶的七点的飞机来昆明，当时在登机的时候遇到超级漂亮的天空，笑着和队友说这是大捷的前兆hh。\n然后来酒店办了入住直接冲向学校了，当时整个人都饿毁了，两荤三素库库炫，也没来得及拍照 。不过云大真的很漂亮，整个云南十二月还是绿叶环绕，本北方人表示哪见过这个。\n然后热身赛直接开打。A题a+b？B题队友猜一发结论直接过了。D题偷听斜对面什么父亲，然后随便一手模发现每个点权值减去父亲权值乘起来直接过了。C题队友一开始思路有点问题，后来lyr测点别的东西我俩直接玩手机了。结果快最后了zmd想到了正解，赛后一问黄队还真对了。\nDAY2 有点紧张！沈阳铜首确实给我们的压力太大了，赛前买了一大批物资，面包，士力架，红牛，赛前满足每位队员的一切合理与不合理的需求(bushi)\n开打！上来队友直接扔给我一道计算几何，说极角排序一下应该就完了。我觉得也是于是上机开始敲。小小调整一下交了结果wa了。此时队友上机写M构造，结果几种方式全都有问题。只能说逆风开局是我们队的常态，甚至比沈阳的台风开局还要好一点。然后zmd开始了无敌节奏，先想出了M超级正确且好写且可证明的思路，直接过了，然后上机又把J题秒了。期间lyr帮忙手推了J题小数据特例并且帮我找到了H题一个小讨论的错误（某人唯二贡献哈哈哈）。然后我上机改改也过了H。赛后想想这个H当时真的很唐，直接用atan2搞出角度其实就完了，根本用不到极角排序。\n然后有点卡题的，当时我说开出CGL其实是应该能稳银的（很棒的前瞻性），于是开始全力开这三题。lyr一直在想L，我看了C就觉得是根号分治，但是一直不知道分完怎么算。zmd张老板直接灵感一现，从结果往回推，轻松过样例！交上去却t了。然后下机想想就找到了问题，问我一个式子，（然后巴拉巴拉交流一堆），改一下直接过了！这个C开的很帅。然后我顺着zmd思路想G题，zmd写了一个记忆化搜索，re了，改了下wa了，好在很快找到反例发现比较难记忆，并且也改不对样例。这时候我就想是不是直接爆搜就完了，上机十分钟写完肯定过样例的，然后试了极限的俩数结果都跑的飞快？于是决定交一发结果直接ac了？？？我真的对这发没报希望啊？最后直接all in L题。zmd和lyr统一了思路，lyr写完交上去wa了，并且我造了一堆样例都没能出错。这时候张老板直接把lyr踹下机开始重构，微微调整过了第一个样例就觉得直接交，结果t了？当时三人盯着那几个while循环瞅，lyr建议关流在交一次（因为debug所以关了关流，交的时候忘开了），结果直接a了！当时张老板直接大喊：“过了！”我们直接下班观战！\n最后稳稳拿银~\n很不错的昆明，最后没有遗憾退役，赛后要了哥哥的签名和合照，美滋滋~\n晚上去一家特色菜馆，超级火爆，最后人均20？\n随便逛逛~\nDAY3 因为行李问题没机会去玩，然后偷了群友拍的鸽子的图片~\n晚上连夜飞回天津，晚上到了已经凌晨了，累鼠了。\nCCPC郑州站、ICPC沈阳站、ecfinal西安站就不转载了，一个是传奇赶路精疲力尽艰难守铜，一个是台风开局铜首结束道心破碎，一个是奖励名额纯纯旅游爽玩西安。\n","date":"2025-05-30T20:02:21+08:00","permalink":"https://mhw-mathcode.github.io/p/2024-icpc-%E6%98%86%E6%98%8E%E5%8C%BA%E5%9F%9F%E8%B5%9B-vlog/","title":"2024 ICPC 昆明区域赛 vlog"},{"content":"实验阶段暂不公开~\n","date":"2025-05-30T11:12:02+08:00","permalink":"https://mhw-mathcode.github.io/p/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E9%87%87%E6%A0%B7-%E6%83%85%E6%84%9F%E6%94%BB%E5%87%BB%E5%AE%9E%E9%AA%8C%E8%BF%9B%E5%BA%A6/","title":"大模型数据采样-情感攻击(实验进度)"},{"content":"前言（一些碎碎念） 高考结束以后什么也不懂的我报考了 cs 专业然后购入了一台集成显卡的电脑……大一的时候还好，只是知道带不动一些大型游戏，好在自己也并不打游戏。然后大二的时候 chatgpt 横空出世迅速占据了日常的生活，深度学习、神经网络直接闯入所有领域，在给各个领域注入新鲜血液与活力的同时，也让大家都多了一门必修课。越来越多的课程引入类似的知识与实验，当时也没有涉及很多大型的数据集与复杂的神经网络，勉强使用电脑的 cpu 运行应对实验。大三上知道了 google colab ，每天都有免费的 gpu 额度使用，甚至充钱可以用到 A100 （当然价格是相当高昂的）。大三下来到了学校的一个实验室做科研实习，LLM 相关的工作对于算力要求实在是很高，而且 colab 一些很不好的体验也难以满足需求，就找学长申请了一个实验室服务器的账号。在这个过程中也遇到了很多困难，也都找到了相应的答案，因此在这里记录一下。\n我使用的是 vscode + filezilla ，vscode 用于远程连接 + coding，filezilla 主要是传输数据。\n远程连接 ssh 首先学长会要你电脑主机的公钥，这篇文章 简洁而且有用，一般生成的公钥-私钥对会在这个路径下 C:/Users/用户名/.ssh 。\n然后使用 vscode 远程连接，需要用到这三个插件。\n然后如同 这篇文章 一样。\n但是我们操作以后可能会遇到一个问题：远程连接xxx失败！返回类似于以下形式的报错信息，也是卡了自己很长时间。\n1 2 3 Failed to parse remote port from server output Exec server for ssh-remote+xx.xx.xxx.xxx failed: Error Error opening exec server for ssh-remote+xx.xx.xx.xx: Error 然后 这篇文章 给出了一个完美的解决方案（膜拜）！总结原因就是 vscode 和 ubuntu 中 glibc 的版本不匹配，但是我们肯定是不好直接更改实验室服务器的系统的，所以对自己的 vscode 降级（降级到1.98及以下）就是最好的方案。\nFilezilla 传输数据 （其实可以直接拖拽完成，但是大规模数据还是用专业的软件比较好）\n然后我在连接之后又出现了一个问题，filezilla 死活连接不上，一直连接超时，然后自己也是摸索出了一个解决办法，因为看到很多博主在连接时会输入密钥的路径，但是自己却没有这个输入的选项，因此觉得应该是这个地方的问题。\n然后选择本地密钥所在的路径即可。\n连接成功以后直接把本地站点的文件拖进远程站点目标的文件夹下即可。\n配置环境 如果是实验室的服务器我觉得可能都已经安装好了Anaconda，并且已经创建并激活好了conda虚拟环境，如果是自己租赁的可能需要自己配置，参考 这篇文章 。\n然后我们直接初始化 python 环境即可。\n1 /opt/anaconda3/bin/conda init bash 重启终端后，会在用户名前出现 (base) 证明成功了，使用 conda info -e 查看 conda 环境。\n然后学长告诉我 /home/用户名 下的空间比较小，大的数据集放在 /data/用户名 。或许我们可以把所有东西（代码还有数据集）都放在 /data/用户名 下，这样会比较方便一点，具体参考 这篇文章 。\n紧接着又出现了一个问题。\nemmm，在研究尝试了一下午无果后，去问了一下学长，然后得到了答案：服务器没网。我应该早点去问学长的www，在学长的帮助下得到了解决方案：\n在服务器终端上运行\n1 export http_proxy=\u0026#34;http://$proxy_ip:$proxy_port\u0026#34; https_proxy=\u0026#34;http://$proxy_ip:$proxy_port\u0026#34; all_proxy=\u0026#34;socks5://$proxy_ip:$proxy_port\u0026#34; 其中 proxy_ip 和 proxy_port 分别代表代理服务器的 IP 地址和端口号（没错，代理服务器的 IP 和端口也是学长给的，我尝试了自己本地的代理没有成功）。\n然后我们直接创建并激活环境\n1 2 conda create -n mypytorch python=3.11 conda activate mypytorch 然后还需要安装最重要的 pytorch ，但是官网最新的版本好像不支持 conda 安装，于是我决定安装上一代版本。\nPyTorch previous-versions\n但是又又又又出现了新的问题：\n大概是因为 pytorch-cuda=12.4 因为版本比较新所以在当前配置的 Conda 通道中不可用，然后官方是提供了支持 CUDA 12.4 的安装命令：\n1 conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia 然后，终于！\n得到了 pytorch 版本号！\ntrick trick (1) 控制使用的显卡 id\n1 2 import os os.environ[\u0026#39;CUDA_VISIBLE_DEVICES\u0026#39;] = \u0026#39;1\u0026#39; trick (2) 使用 jupyter\n终端执行命令 conda install jupyter\nvscode 中安装插件 jupyter\ntrick (3) 在连接服务器的 vscode 上使用 copilot\n参考 这篇文章 ，秒解决~\ntrick (4) 在本地通过 py 代码控制服务器运行指令或者下载文件与文件夹\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import paramiko from scp import SCPClient # 连接参数 hostname = \u0026#39;\u0026#39; # 云服务器 IP port = 22 # 一般是 22 username = \u0026#39;\u0026#39; # 云服务器用户名（如 root） # password = \u0026#39;your_password\u0026#39; # 云服务器密码 def control(command=None): # 创建 SSH 客户端 ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(hostname, port, username) # 控制服务器执行远程命令 try: stdin, stdout, stderr = ssh.exec_command(command) except Exception as e: print(f\u0026#34;命令执行失败: {e}\u0026#34;) out, err = stdout.read().decode(), stderr.read().decode() ssh.close() return out, err def download(): # 创建SCP客户端 ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(hostname, port, username) scp = SCPClient(ssh.get_transport()) # 下载文件与文件夹 try: # 想要下载的文件或者文件夹 下载的目录 scp.get(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;) scp.get(\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, recursive=True) print(\u0026#34;文件下载成功\u0026#34;) except Exception as e: print(f\u0026#34;文件下载失败: {e}\u0026#34;) # 关闭SCP和SSH连接 scp.close() ssh.close() 虚拟机联网（小插曲） 当时试了很多方法也没有让服务器连接上网络，于是我决定去自己的虚拟机上先配置好深度学习的环境，然后把环境文件直接传输到服务器上。然后很逆天的发现，自己的虚拟机也连不上网络？然后有试了很多乱七八糟的方法，直到我看到了 这篇文章 ，直接还原默认设置，完美解决了！\n可以看到后面直接 ping baidu.com 正在顺利的进行着。\n","date":"2025-05-26T18:25:36+08:00","permalink":"https://mhw-mathcode.github.io/p/vscode-%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","title":"Vscode 远程连接实验室服务器训练模型"},{"content":"hello world! ","date":"2025-01-21T10:09:52+08:00","permalink":"https://mhw-mathcode.github.io/p/myfirstblog/","title":"MyFirstBlog"}]