<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine Learning on mhw</title>
        <link>https://mhw-mathcode.github.io/tags/machine-learning/</link>
        <description>Recent content in Machine Learning on mhw</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>mhw-mathcode</copyright>
        <lastBuildDate>Fri, 08 Aug 2025 10:26:26 +0000</lastBuildDate><atom:link href="https://mhw-mathcode.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>学习笔记-机器学习</title>
        <link>https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Mon, 16 Jun 2025 19:29:56 +0800</pubDate>
        
        <guid>https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;h2 id=&#34;0-算法速览&#34;&gt;0 算法速览
&lt;/h2&gt;&lt;p&gt;监督学习算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性回归（Linear Regression）：用于回归任务，预测连续的数值。&lt;/li&gt;
&lt;li&gt;逻辑回归（Logistic Regression）：用于二分类任务，预测类别。&lt;/li&gt;
&lt;li&gt;决策树（Decision Tree）：基于树状结构进行决策的分类或回归方法。&lt;/li&gt;
&lt;li&gt;支持向量机（SVM）：用于分类任务，构建超平面进行分类。&lt;/li&gt;
&lt;li&gt;K近邻算法&lt;/li&gt;
&lt;li&gt;集成学习&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;无监督学习算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K-means 聚类：通过聚类中心将数据分组。&lt;/li&gt;
&lt;li&gt;主成分分析（PCA）：用于降维，提取数据的主成分。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;01-线性回归&#34;&gt;0.1 线性回归
&lt;/h3&gt;&lt;p&gt;线性回归 (Linear Regression) 是一种用于预测连续值的最基本的机器学习算法，它假设目标变量 y 和特征变量 x 之间存在线性关系，并试图找到一条最佳拟合直线来描述这种关系。&lt;/p&gt;
&lt;p&gt;常用的误差函数是均方误差 (MSE) : $MSE = 1/n * Σ(y_i - y_{pred_i})^2$&lt;/p&gt;
&lt;h4 id=&#34;求解方法-最小二乘法&#34;&gt;求解方法-最小二乘法
&lt;/h4&gt;&lt;p&gt;最小二乘法的目标是最小化残差平方和（RSS），其公式为：$RSS = \sum_{i=1}^n(y_i - \hat{y}_i)^2$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1.jpg&#34;
	width=&#34;1440&#34;
	height=&#34;1080&#34;
	srcset=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1_hu_dc3fae64383c103d.jpg 480w, https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1_hu_cb0bc3cbb612de23.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推导&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;得到最佳的 $w, b$&lt;/p&gt;
$$
\begin{bmatrix} w \\ b \end{bmatrix} = \begin{bmatrix} \sum_{i=1}^n x_i^2 &amp; \sum_{i=1}^n x_i \\ \sum_{i=1}^n x_i &amp; n \end{bmatrix}^{-1} \begin{bmatrix} \sum_{i=1}^n x_i y_i \\ \sum_{i=1}^n y_i \end{bmatrix}
$$&lt;h4 id=&#34;求解方法-梯度下降法&#34;&gt;求解方法-梯度下降法
&lt;/h4&gt;&lt;p&gt;梯度下降法的目标是最小化损失函数 $J(w,b)$ 。对于线性回归问题，通常使用均方误差（MSE）作为损失函数：&lt;/p&gt;
$$ J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 $$&lt;p&gt;参数更新：&lt;/p&gt;
$$ w := w - \alpha \frac{\partial J}{\partial w} \quad b := b - \alpha \frac{\partial J}{\partial b} $$&lt;p&gt;梯度下降法的步骤&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化参数：初始化 w 和 b 的值（通常设为 0 或随机值）。&lt;/li&gt;
&lt;li&gt;计算损失函数：计算当前参数下的损失函数值 $J(w,b)$ 。&lt;/li&gt;
&lt;li&gt;计算梯度：计算损失函数对 w 和 b 的偏导数。&lt;/li&gt;
&lt;li&gt;更新参数：根据梯度更新 w 和 b。&lt;/li&gt;
&lt;li&gt;重复迭代：重复步骤 2 到 4，直到损失函数收敛或达到最大迭代次数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;02-逻辑回归&#34;&gt;0.2 逻辑回归
&lt;/h3&gt;&lt;p&gt;逻辑回归（Logistic Regression）是一种广泛应用于分类问题的统计学习方法，尽管名字中带有&amp;quot;回归&amp;quot;，但它实际上是一种用于二分类或多分类问题的算法。&lt;/p&gt;
&lt;p&gt;逻辑回归通过使用逻辑函数（也称为 Sigmoid 函数）将线性回归的输出映射到 0 和 1 之间，从而预测某个事件发生的概率。建立模型：&lt;/p&gt;
\[
p(y = 1|X) = \sigma(w^T X + b)
\]&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( X \) 是输入特征（可以是多个特征组成的向量）。&lt;/li&gt;
&lt;li&gt;\( w \) 是权重向量。&lt;/li&gt;
&lt;li&gt;\( b \) 是偏置项。&lt;/li&gt;
&lt;li&gt;\(\sigma(z) = \frac{1}{1+e^{-z}}\) 是Sigmoid函数。Sigmoid函数将模型的输出值 \((w^T X + b)\) 映射到0到1之间，因此它可以看作是属于类别1的概率。注意 $\sigma&amp;rsquo;(z) = \sigma(z)(1 - \sigma(z))$ 。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用对数损失函数&lt;/p&gt;
\[ L(\theta) = 
  \begin{cases} 
   -\log(p), &amp; \text{if } y = 1 \\
   -\log(1-p), &amp; \text{if } y = 0 
  \end{cases}
\]&lt;p&gt;合并得到单个样品的损失函数&lt;/p&gt;
\[ 
    L(\theta) = -ylog(p)-(1-y)log(1-p)
\]&lt;p&gt;因此总体损失函数（也就是交叉熵损失函数）&lt;/p&gt;
\[ L(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(p^{(i)}) + (1 - y^{(i)}) \log(1 - p^{(i)}) \right] \]&lt;h4 id=&#34;求解方法-梯度下降法-1&#34;&gt;求解方法-梯度下降法
&lt;/h4&gt;&lt;p&gt;对 \(w\) 的梯度：
&lt;/p&gt;
\[ \frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \]&lt;p&gt;对 \(b\) 的梯度：
&lt;/p&gt;
\[ \frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \]&lt;p&gt;&lt;img src=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2.jpg&#34;
	width=&#34;1279&#34;
	height=&#34;1706&#34;
	srcset=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2_hu_e9d3f2f7cdc08657.jpg 480w, https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2_hu_1c4e3fb87193097c.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推导&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;74&#34;
		data-flex-basis=&#34;179px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;03-决策树&#34;&gt;0.3 决策树
&lt;/h3&gt;&lt;p&gt;决策树（Decision Tree），它是一种以树形数据结构来展示决策规则和分类结果的模型，作为一种归纳学习算法，其重点是将看似无序、杂乱的已知数据，通过某种技术手段将它们转化成可以预测未知数据的树状模型，每一条从根结点（对最终分类结果贡献最大的属性）到叶子结点（最终分类结果）的路径都代表一条决策的规则。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.png&#34;
	width=&#34;754&#34;
	height=&#34;561&#34;
	srcset=&#34;https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3_hu_690baba9cb1d5aaf.png 480w, https://mhw-mathcode.github.io/p/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3_hu_c7dbe716591b324a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;决策树构建过程&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;322px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;在这个过程中，寻找最优划分属性是决策树过程中的重点，那么应该如何求解呢？&lt;/p&gt;
&lt;h4 id=&#34;求解方法-信息增益&#34;&gt;求解方法-信息增益
&lt;/h4&gt;&lt;p&gt;首先引入信息熵的概念&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;信息熵：描述随机变量的不确定性（也就是混乱程度）。
假设某随机变量的概率分布为：$P(X=x_i) = p_i, \quad i = 1, 2, \ldots, n$ ，则它的信息熵计算公式为：$H(X) = - \sum_{i=1}^{n} p_i \log p_i$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;在决策树中，信息熵&lt;/p&gt;
\[ H(D) = - \sum_{k=1}^{K} \frac{|D_k|}{|D|} \log \frac{|D_k|}{|D|} \]&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( D \)：整个数据集&lt;/li&gt;
&lt;li&gt;\( D_k \)：第 \( k \) 个类的样本子集&lt;/li&gt;
&lt;li&gt;\( \frac{|D_k|}{|D|} \)：第 \( k \) 类的概率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;条件熵&lt;/p&gt;
\[ H(D|A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i) \]&lt;p&gt;\( H(D_i) \) 是每个子集 \( D_i \) 的信息熵&lt;/p&gt;
\[ H(D_i) = - \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} \log \frac{|D_{ik}|}{|D_i|} \]&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( A \)：某个属性&lt;/li&gt;
&lt;li&gt;\( D_i \)：属性 \( A \) 的第 \( i \) 个取值所对应的数据子集&lt;/li&gt;
&lt;li&gt;\( D_{ik} \)：在第 \( i \) 个子集中属于第 \( k \) 类的样本数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此有&lt;/p&gt;
\[ H(D|A) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} \log \frac{|D_{ik}|}{|D_i|} \]&lt;p&gt;特征 \( A \) 对训练数据集 \( D \) 的信息增益 \( gain(D, A) \) 定义为集合 \( D \) 的信息熵 \( H(D) \) 与特征 \( A \) 给定条件下 \( D \) 的信息条件熵 \( H(D|A) \) 之差，即公式为：&lt;/p&gt;
\[ gain(D, A) = H(D) - H(D|A) \]&lt;p&gt;信息增益表示得知特征 \( X \) 的信息而使得类 \( Y \) 的信息的不确定性减少的程度，因此信息增益最大的属性就是最优划分属性，标志性算法 $ID3$ 。&lt;/p&gt;
&lt;h4 id=&#34;求解方法-增益比&#34;&gt;求解方法-增益比
&lt;/h4&gt;&lt;p&gt;信息增益虽然在理论上可以找到最优的划分属性，但在某些情况下会存在问题。信息增益比较偏好可取值较多的属性。因此为了矫正信息增益偏好的问题，使算法不偏向可取值较多的属性，引申出了增益比的思想。&lt;/p&gt;
\[ Gain\_ratio(D, A) = \frac{Gain(D, A)}{H(A)} \]&lt;p&gt;可以看出，增益比就是信息增益除以属性 $A$ 的信息熵，当属性 $A$ 可取值增多的时候，$H(A)$ 一般也增大，因此在一定程度上能抑制信息增益偏好取值多的属性的特点，但是增益比偏好取值较少的属性。&lt;/p&gt;
&lt;p&gt;算法 $C4.5$ 是算法 $ID3$ 的改进版，它使用了信息增益和增益比两种选择算法，先选出信息增益高于平均水平的属性，然后再在这些属性中选择增益比最高的，作为最优划分属性。这样综合了信息增益和增益比的优点，可以取得较好的效果。&lt;/p&gt;
&lt;h4 id=&#34;求解方法-基尼指数&#34;&gt;求解方法-基尼指数
&lt;/h4&gt;&lt;p&gt;基尼指数是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。这个规律与信息熵的相同。&lt;/p&gt;
\[ Gini(D) = \sum_{k=1}^{n} \sum_{k&#39; \neq k} p_k p_{k&#39;} = 1 - \sum_{k=1}^{n} p_k^2 \]&lt;p&gt;使用基尼指数来选择最优划分属性也是对比不同属性划分后基尼指数的差值，选择使样本集基尼指数减小最多的属性。&lt;/p&gt;
\[ Gain(D, a) = Gini(D) - \sum_{i=1}^{n} \frac{|D^i|}{|D|} Gini(D^i) \]&lt;p&gt;著名的 $CART$ 决策树就是使用基尼指数来作为划分准则， $CART$ 决策树与 $ID3$ 和 $C4.5$ 的区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;划分准则不同，CART决策树使用基尼指数， $ID3$ 和 $C4.5$ 使用信息熵。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$ID3$ 和 $C4.5$ 划分时，一个节点可以划分为多个子结点，子结点数量根据属性可取值的数量决定。而 $CART$ 决策树是严格的二叉树结构，就是说 $1$ 个节点最多划分为 $2$ 子结点。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;04-支持向量机&#34;&gt;0.4 支持向量机
&lt;/h3&gt;&lt;p&gt;支持向量机（Support Vector Machine，简称 SVM）是一种监督学习算法，主要用于分类和回归问题。&lt;/p&gt;
&lt;p&gt;SVM 的核心思想是找到一个最优的超平面，将不同类别的数据分开。这个超平面不仅要能够正确分类数据，还要使得两个类别之间的间隔（margin）最大化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;支持向量：支持向量是离超平面最近的样本点。这些支持向量对于定义超平面至关重要。支持向量机通过最大化支持向量到超平面的距离（即最大化间隔）来选择最佳的超平面。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;当训练样本&lt;strong&gt;线性可分&lt;/strong&gt;时，通过&lt;strong&gt;硬间隔最大化&lt;/strong&gt;，学习一个&lt;strong&gt;线性可分支持向量机&lt;/strong&gt;；&lt;/p&gt;
&lt;p&gt;当训练样本&lt;strong&gt;近似线性可分&lt;/strong&gt;时，通过&lt;strong&gt;软间隔最大化&lt;/strong&gt;，学习一个&lt;strong&gt;线性支持向量机&lt;/strong&gt;；&lt;/p&gt;
&lt;p&gt;当训练样本&lt;strong&gt;线性不可分&lt;/strong&gt;时，通过&lt;strong&gt;核技巧和软间隔最大化&lt;/strong&gt;，学习一个&lt;strong&gt;非线性支持向量机&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;求解方法-间隔最大化和支持向量&#34;&gt;求解方法-间隔最大化和支持向量
&lt;/h4&gt;&lt;p&gt;好难……&lt;/p&gt;
&lt;h3 id=&#34;05-k近邻算法&#34;&gt;0.5 K近邻算法
&lt;/h3&gt;&lt;p&gt;K 近邻算法（K-Nearest Neighbors，简称 KNN）是一种简单且常用的分类和回归算法。&lt;/p&gt;
&lt;p&gt;K 近邻算法属于监督学习的一种，核心思想是通过计算待分类样本与训练集中各个样本的距离，找到距离最近的 K 个样本，然后根据这 K 个样本的类别或值来预测待分类样本的类别或值。&lt;/p&gt;
&lt;h4 id=&#34;求解方法&#34;&gt;求解方法
&lt;/h4&gt;&lt;p&gt;基本步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;计算距离：计算待分类样本与训练集中每个样本的距离。常用的距离度量方法有欧氏距离、曼哈顿距离等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;选择 K 个最近邻：根据计算出的距离，选择距离最近的 K 个样本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;投票或平均：对于分类问题，K 个最近邻中出现次数最多的类别即为待分类样本的类别；对于回归问题，K 个最近邻的值的平均值即为待分类样本的值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;06-集成学习&#34;&gt;0.6 集成学习
&lt;/h3&gt;&lt;h3 id=&#34;07-k-means-聚类&#34;&gt;0.7 K-means 聚类
&lt;/h3&gt;&lt;p&gt;K-means 聚类是一种常用的基于距离的聚类算法，旨在将数据集划分为 K 个簇。算法的目标是最小化簇内的点到簇中心的距离总和。&lt;/p&gt;
&lt;h4 id=&#34;求解方法-1&#34;&gt;求解方法
&lt;/h4&gt;&lt;p&gt;基本步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;选择 $K$ 值：设定簇的数量 。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;初始化簇中心：随机选择 $K$ 个数据点作为初始簇中心（centroids）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分配步骤（Assignment Step）：对于数据集中的每个点，将它分配到最近的簇中心对应的簇。这里的“距离”通常使用欧氏距离（Euclidean distance）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新步骤（Update Step）：根据当前的簇分配，重新计算每个簇的中心，即计算簇内所有点的均值作为新的簇中心。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重复 3 和 4 步：不断重复分配和更新步骤，直到簇中心不再发生变化（收敛）或达到指定的最大迭代次数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;确定最佳的簇数 $K$ 是 $K-means$ 聚类中的一个难点。聚类的目标是使得每个样本点到距离其最近的聚类中心的总误差平方和（也即聚类的代价函数，记作 $SSE$ ）尽可能小。&lt;/p&gt;
&lt;p&gt;空间中数据对象与聚类中心间的欧式距离计算公式为：&lt;/p&gt;
\[ d(x, C_i) = \sqrt{\sum_{j=1}^{m} (x_j - C_{ij})^2} \]&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( x \) 为数据对象，&lt;/li&gt;
&lt;li&gt;\( C_i \) 为第 \( i \) 个聚类中心，&lt;/li&gt;
&lt;li&gt;\( m \) 为数据对象的维度，&lt;/li&gt;
&lt;li&gt;\( x_j \)，\( C_{ij} \) 为 \( x \) 和 \( C_i \) 的第 \( j \) 个属性值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整个数据集的误差平方和 SSE 计算公式为：&lt;/p&gt;
\[ SSE = \sum_{i=1}^{k} \sum_{x \in C_i} |d(x, C_i)|^2 \]&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSE 的大小表示聚类结果的好坏，&lt;/li&gt;
&lt;li&gt;\( k \) 为簇的个数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理论上随着 $K$ 的增加， $SSE$ 会单调递减，当 $K$ 超过某一个数后，每个类簇的聚合程度不再获得显著提升，此时我们就可以认为已找到最佳 $K$ 的取值（肘部法）。&lt;/p&gt;
&lt;h4 id=&#34;k-means&#34;&gt;K-means++
&lt;/h4&gt;&lt;p&gt;K-means++ 是一种改进的初始化方法，可以帮助选择更合理的初始中心，优先选择“距离最远”的点作为初始质心，减少陷入局部最优的风险。&lt;/p&gt;
&lt;p&gt;基本步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;从数据集 $\mathcal{X}$ 中随机（均匀分布）选取一个样本点作为第一个初始聚类中心;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接着计算每个样本与当前已有聚类中心之间的最短距离，用 $D(x)$ 表示；然后计算每个样本点被选为下一个聚类中心的概率 $P(x) = \frac{D(x)^2}{\sum_{x \in \mathcal{X}} D(x)^2}$，最后选择最大概率值（或者概率分布）所对应的样本点作为下一个簇中心；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重复步骤 2，直到选择 $K$ 个聚类中心&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;优势：避免随机初始化，加快收敛速度，聚类结果更加稳定。&lt;/p&gt;
&lt;h3 id=&#34;08-主成分分析&#34;&gt;0.8 主成分分析
&lt;/h3&gt;&lt;p&gt;主成分分析（PCA）是一种无监督学习方法，旨在通过线性变换将原始的高维数据映射到一个低维空间，同时尽可能保留数据的方差（即信息量）。简单来说，PCA 的目标是找到一组新的坐标轴（称为主成分），这些坐标轴能够捕捉数据中最大的变异性，并用更少的维度来近似表示原始数据。&lt;/p&gt;
&lt;h4 id=&#34;求解方法-2&#34;&gt;求解方法
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;数据中心化：首先将数据中心化，即让每个特征的均值变为 0。&lt;/li&gt;
&lt;li&gt;计算协方差矩阵：&lt;/li&gt;
&lt;/ol&gt;
\[ \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y}) \]&lt;p&gt;协方差为正时，说明X和Y是正相关关系；协方差为负时，说明X和Y是负相关关系；协方差为0时，说明X和Y是相互独立。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;特征值分解：对协方差矩阵进行特征值分解，得到主成分的方向（特征向量）和重要性（特征值）。令\( A \) 是协方差矩阵，\( \lambda \) 是特征值，\( I \) 是单位矩阵，求解 \[ \det(A - \lambda I) = 0 \]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;得到特征值 \( \lambda \) 以后代入 $(A - \lambda I)v_1 = 0$ 解得特征向量 $v_1$ 。&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;排序和选择主成分：将特征值从大到小排序，特征值最大的为第一个主成分，捕捉了数据中最大的变化，也就是数据分布中最显著的变化方向。第二个主成分与第一个主成分正交（相互垂直），且在正交约束下方差次大的方向。后续主成分：依此类推，每个主成分都与前面的主成分正交，并按特征值大小递减排列。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;投影数据：将中心化后的数据投影到第一个主成分上，得到降维后的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;一些问题&#34;&gt;一些问题
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;为什么要计算协方差矩阵？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PCA 的目标是找到一组新的坐标轴（称为主成分），使得数据在这些轴上的投影方差最大化，同时这些轴相互正交（不相关）。协方差矩阵正好量化了数据中的变异性和变量间的相关性，我们可以了解到哪些变量变化较大，哪些变量之间存在较强的关联，为找到这样的轴提供了基础。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;为什么要进行特征值分解？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;特征值表示每个特征向量方向上的方差大小。特征值越大，说明该方向捕捉的变异性越多。通过特征值分解，我们可以将原始数据投影到这些特征向量上，从而实现降维，同时尽可能保留数据的信息。&lt;/p&gt;
&lt;h2 id=&#34;1-引言&#34;&gt;1 引言
&lt;/h2&gt;&lt;h3 id=&#34;11-什么是机器学习&#34;&gt;1.1 什么是机器学习
&lt;/h3&gt;&lt;p&gt;一个好的学习问题定义如下：一个程序被认为能从经验 E 中学习，解决任务 T ，达到性能度量值 P ，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。&lt;/p&gt;
&lt;p&gt;目前存在几种不同类型的学习算法，其中主要的两种类型被我们称之为：&lt;strong&gt;监督学习&lt;/strong&gt;和&lt;strong&gt;无监督学习&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;12-监督学习&#34;&gt;1.2 监督学习
&lt;/h3&gt;&lt;p&gt;监督学习：给定带有标签的数据，模型通过学习输入和标签之间的关系来做预测。&lt;/p&gt;
&lt;p&gt;回归 (regression) 问题：推测出这一系列连续值属性。&lt;/p&gt;
&lt;p&gt;分类 (classification) 问题：推测出离散的输出值。&lt;/p&gt;
&lt;h3 id=&#34;13-无监督学习&#34;&gt;1.3 无监督学习
&lt;/h3&gt;&lt;p&gt;无监督学习：没有标签的数据，模型通过探索数据中的结构或模式来进行学习。&lt;/p&gt;
&lt;p&gt;聚类算法：将数据集划分成两个不同的簇。&lt;/p&gt;
&lt;p&gt;鸡尾酒算法：分离两种声音。（一个具体实例，仅仅只需要一行代码实现）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;svd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repmat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;2-单变量线性回归-linear-regression-with-one-variable&#34;&gt;2 单变量线性回归 (Linear Regression with One Variable)
&lt;/h2&gt;</description>
        </item>
        
    </channel>
</rss>
